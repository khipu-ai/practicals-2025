{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n--KTVI-BkW"
      },
      "source": [
        "# **JAX Basics: NumPy on Steroids ü¶æ**\n",
        "\n",
        "Alright, let's dive into JAX! This section will equip you with the foundational JAX knowledge you'll need for the rest of the practicals. Think of JAX as NumPy's cooler, faster cousin, especially when it comes to machine learning.\n",
        "\n",
        "**Why JAX?**\n",
        "\n",
        "JAX is a powerful Python library that turbocharges numerical computation and machine learning.  It's like NumPy but with superpowers:\n",
        "\n",
        "*   üöÄ **Blazing Fast:**  JAX uses just-in-time (JIT) compilation to speed up your code, making it run much faster, especially on GPUs and TPUs. Say goodbye to slow loops!\n",
        "*   üí° **Automatic Differentiation:**  JAX can automatically calculate gradients of your functions (using `jax.grad`). This is ESSENTIAL for training neural networks and other ML models, and JAX makes it super easy!\n",
        "*   ‚ú® **Auto-vectorization & Parallelization:**  JAX can automatically vectorize and parallelize your code across multiple cores or devices (GPUs, TPUs) with minimal effort (using `jax.vmap` and `jax.pmap`).\n",
        "\n",
        "We'll explore these superpowers in this section!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JhOquVQ-BkY"
      },
      "source": [
        "<details>\n",
        "  <summary>Dive Deeper: What is JAX Under the Hood?</summary>\n",
        "  <p>\n",
        "      JAX achieves its magic by combining two key technologies:\n",
        "      \n",
        "      *   **Autograd:**  For automatic differentiation (calculating gradients).\n",
        "      *   **XLA (Accelerated Linear Algebra):** For just-in-time compilation and efficient execution, especially on accelerators like GPUs and TPUs.\n",
        "      \n",
        "      These technologies allow JAX to perform numerical computations with incredible speed and flexibility. JAX encourages a **functional programming style**, where functions are pure and transformations are composable, leading to cleaner and more efficient code.\n",
        "  </p>\n",
        "  <p>\n",
        "      While JAX is lower-level than libraries like PyTorch or TensorFlow, its simplicity and powerful primitives make it a favorite for research and for building custom ML systems. For building neural networks, JAX is often used with higher-level libraries like Flax or Haiku, which provide more structure and convenience.\n",
        "  </p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iz1gkPB-BkY"
      },
      "source": [
        "## 2.1 JAX ‚ù§Ô∏è NumPy: A Happy Family (10 minutes)\n",
        "\n",
        "Good news! If you already know NumPy, you're in great shape to learn JAX. JAX NumPy (`jax.numpy` or `jnp`) is very similar to NumPy. Most of the NumPy functions you're familiar with work the same way in JAX!\n",
        "\n",
        "Let's explore the similarities and differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n33h9NE9-BkY"
      },
      "source": [
        "### Similarities ‚úÖ: NumPy Comfort in JAX\n",
        "\n",
        "JAX NumPy aims to be a drop-in replacement for NumPy for many common operations. Let's see some examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKO1T1-s-BkZ",
        "outputId": "6ad90bb8-88d8-41ef-890a-166073ede786"
      },
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def print_comparison(title, np_result, jax_result):\n",
        "    print(f\"\\n**{title}**\") # Markdown formatting for titles!\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"NumPy: {np_result}\")\n",
        "    print(f\"JAX:   {jax_result}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Create arrays for use in examples\n",
        "np_array1, np_array2 = np.array([1, 2, 3]), np.array([4, 5, 6])\n",
        "jax_array1, jax_array2 = jnp.array([1, 2, 3]), jnp.array([4, 5, 6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Element-wise Addition**\n",
            "========================================\n",
            "NumPy: [5 7 9]\n",
            "JAX:   [5 7 9]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxUrb25I-BkZ"
      },
      "source": [
        "#### ‚ûï Element-wise Addition: `+`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t8_Lnxm-BkZ",
        "outputId": "cf348b36-b15f-4e6d-91dd-d700d0f6ef45"
      },
      "source": [
        "np_add = np_array1 + np_array2\n",
        "jax_add = jax_array1 + jax_array2\n",
        "print_comparison(\"Element-wise Addition\", np_add, jax_add)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Element-wise Addition**\n",
            "========================================\n",
            "NumPy: [5 7 9]\n",
            "JAX:   [5 7 9]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4WUSsBd-Bka"
      },
      "source": [
        "#### üî™ Indexing and Slicing: `[]`\n",
        "\n",
        "Accessing parts of an array using indices and slices is the same:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaANg0E6-Bka",
        "outputId": "86c7562f-55cc-4c5f-cc9d-acf0dff47567"
      },
      "source": [
        "np_slice = np_array1[1:4] # Get elements from index 1 up to (but not including) 4\n",
        "jax_slice = jax_array1[1:4]\n",
        "print_comparison(\"Array Slicing\", np_slice, jax_slice)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Array Slicing**\n",
            "========================================\n",
            "NumPy: [2 3]\n",
            "JAX:   [2 3]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2trO_bx-Bka"
      },
      "source": [
        "#### ‚û°Ô∏è Array Concatenation: `.concatenate`\n",
        "\n",
        "Combining arrays is also familiar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXiANPhD-Bka",
        "outputId": "001d31e6-e289-4488-f47c-d01126dcc0cf"
      },
      "source": [
        "np_concat = np.concatenate([np_array1, np_array2])\n",
        "jax_concat = jnp.concatenate([jax_array1, jax_array2])\n",
        "print_comparison(\"Array Concatenation\", np_concat, jax_concat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Array Concatenation**\n",
            "========================================\n",
            "NumPy: [1 2 3 4 5 6]\n",
            "JAX:   [1 2 3 4 5 6]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MH64U9E-Bka"
      },
      "source": [
        "#### ‚úñÔ∏è Matrix Multiplication: `.dot`\n",
        "\n",
        "Matrix multiplication works the same way for linear algebra operations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGv__Yfy-Bkb",
        "outputId": "75b9eb2d-22b2-4be9-d8d5-104fd96ab441"
      },
      "source": [
        "# Create matrices for example\n",
        "np_matrix = np.array([[1, 2], [3, 4]])\n",
        "jax_matrix = jnp.array([[1, 2], [3, 4]])\n",
        "\n",
        "np_matmul = np.dot(np_matrix, np_matrix)\n",
        "jax_matmul = jnp.dot(jax_matrix, jax_matrix)\n",
        "print_comparison(\"Matrix Multiplication\", np_matmul, jax_matmul)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Matrix Multiplication**\n",
            "========================================\n",
            "NumPy: [[ 7 10]\n",
            " [15 22]]\n",
            "JAX:   [[ 7 10]\n",
            " [15 22]]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbRVkLYg-Bkb"
      },
      "source": [
        "#### üßÆ Mathematical Operations: `.sin`, `.mean`, etc.\n",
        "\n",
        "Many mathematical functions like `sin`, `cos`, `exp`, `mean`, `sum`, etc., are also available in JAX NumPy and behave similarly to their NumPy counterparts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Jg1bXnJb-Bkb",
        "outputId": "0bdbbb07-893b-4259-80f9-6d6a12f80e81"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate x values\n",
        "x = np.linspace(0, 2*np.pi, 100)\n",
        "\n",
        "# Calculate sine using NumPy\n",
        "y_np = np.sin(x)\n",
        "\n",
        "# Calculate sine using JAX\n",
        "y_jax = jnp.sin(x)\n",
        "\n",
        "# Plotting (same plotting code for both)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y_np, label='NumPy sin(x)', color='blue')\n",
        "plt.plot(x, y_jax, label='JAX sin(x)', color='red', linestyle='--')\n",
        "plt.title('Sine Function: NumPy vs JAX')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('sin(x)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE5dlnwd-Bkb"
      },
      "source": [
        "### Differences ‚ùå: Where JAX and NumPy Diverge\n",
        "\n",
        "While JAX NumPy is designed to be very similar to NumPy, there are some key differences you need to be aware of, especially when writing ML code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH9NJo5Q-Bkb"
      },
      "source": [
        "#### 1. üö´ Immutability: JAX Arrays Can't Be Changed in Place\n",
        "\n",
        "Unlike NumPy arrays, JAX arrays are **immutable**. This means once you create a JAX array, you cannot modify its values directly.  Trying to change an element in-place will raise an error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOT2AOyi-Bkb"
      },
      "source": [
        "Let's see what happens when we try to change the first element of a NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLMDTe2O-Bkb",
        "outputId": "466785cd-21bc-4efe-af9b-a3c432d09947"
      },
      "source": [
        "# NumPy: Mutable Arrays (Changes are allowed)\n",
        "x_np = np.arange(10)\n",
        "print(f\"Original NumPy array: {x_np}\")\n",
        "\n",
        "# Change the first element of x_np\n",
        "x_np[0] = 10  # In-place modification - this works!\n",
        "print(f\"Modified NumPy array: {x_np}\") # NumPy array is changed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original NumPy array: [0 1 2 3 4 5 6 7 8 9]\n",
            "Modified NumPy array: [10  1  2  3  4  5  6  7  8  9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6YumzOh-Bkb"
      },
      "source": [
        "Now, let's try the same with a JAX array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm3WjwyK-Bkb",
        "outputId": "8bdd85c4-2cbf-49b3-ddf2-246c9cd6116a"
      },
      "source": [
        "# JAX: Immutable Arrays (Changes are NOT allowed)\n",
        "try:\n",
        "    x_jax = jnp.arange(10)\n",
        "    print(f\"Original JAX array: {x_jax}\")\n",
        "    x_jax[0] = 10  # In-place modification - this will cause an error!\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\") # JAX array remains unchanged"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original JAX array: [0 1 2 3 4 5 6 7 8 9]\n",
            "Error: '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKr7hd8X-Bkb"
      },
      "source": [
        "See? It fails!  JAX throws an error because it doesn't allow in-place modifications.\n",
        "\n",
        "**Updating JAX Arrays Immutably:**\n",
        "\n",
        "To update JAX arrays, you use special \"functional update\" methods like `.at[index].set(value)`. These methods don't modify the original array; instead, they return a *new*, updated copy of the array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtbGmEdB-Bkb",
        "outputId": "a574edb8-2a0e-4937-de4c-1c7a13ad6a2b"
      },
      "source": [
        "x_jax = jnp.arange(10)\n",
        "print(f\"Original JAX array:  {x_jax}\")\n",
        "\n",
        "new_x_jax = x_jax.at[0].set(10) # Creates a *new* array with the change\n",
        "print(f\"New JAX array (modified): {new_x_jax}\") # 'new_x_jax' is modified\n",
        "print(f\"Original JAX array (unchanged): {x_jax}\") # 'x_jax' is still the same"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original JAX array:  [0 1 2 3 4 5 6 7 8 9]\n",
            "New JAX array (modified): [10  1  2  3  4  5  6  7  8  9]\n",
            "Original JAX array (unchanged): [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpskPWSy-Bkc"
      },
      "source": [
        "**Key Takeaway:**  Remember to use functional updates like `.at[].set()` when you need to modify JAX arrays. Functional programming is a core principle in JAX and enables its powerful transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mlbbSov-Bkc"
      },
      "source": [
        "#### 2. üé≤ Explicit Randomness: JAX Makes You Manage Randomness\n",
        "\n",
        "JAX handles random number generation differently than NumPy. In JAX, randomness is **explicit**.  You need to manage random \"keys\" to control random number generation, making your code more reproducible and predictable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUNnPKaR-Bkc"
      },
      "source": [
        "Let's compare NumPy's implicit randomness to JAX's explicit approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veZRdBhg-Bkc",
        "outputId": "20cb3d1c-5cbb-4171-a942-d90fbe1fc63c"
      },
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "# NumPy: Implicit Randomness (Global Seed)\n",
        "np.random.seed(0) # Set seed globally\n",
        "np_rand = np.random.rand(3) # Generate random numbers\n",
        "print(f\"NumPy Random: {np_rand}\")\n",
        "\n",
        "# JAX: Explicit Randomness (Using Keys)\n",
        "key = random.PRNGKey(0) # Create a random key\n",
        "jax_rand = random.uniform(key, (3,)) # Pass key to random function\n",
        "print(f\"JAX Random:   {jax_rand}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Random: [0.5488135  0.71518937 0.60276338]\n",
            "JAX Random:   [0.9653214  0.31468165 0.63302994]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGWl42cy-Bkc"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "*   **NumPy:** NumPy uses a global random number generator. You set the seed once using `np.random.seed()`, and subsequent calls to NumPy random functions will use that seed implicitly.\n",
        "*   **JAX:** JAX uses explicit random keys. You create a key using `random.PRNGKey()` and then explicitly pass that key to each JAX random function (like `random.uniform`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzyyjLFY-Bkc"
      },
      "source": [
        "**Generating Multiple Sets of Random Numbers in JAX:**\n",
        "\n",
        "If you need multiple sets of independent random numbers in JAX, you must **split** the key each time you need randomness. This ensures that each random operation gets a fresh, independent source of randomness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOQXTo9U-Bkc",
        "outputId": "947b7d97-d64c-467c-dccd-1929d364fbc8"
      },
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "# NumPy: Multiple Random Sets (Implicit - can be less reproducible in complex code)\n",
        "np.random.seed(0)\n",
        "np_rand_set1 = np.random.rand(3)\n",
        "np_rand_set2 = np.random.rand(3)\n",
        "print(f\"NumPy Random Set 1: {np_rand_set1}\")\n",
        "print(f\"NumPy Random Set 2: {np_rand_set2}\")\n",
        "\n",
        "# JAX: Multiple Random Sets (Explicit and Reproducible)\n",
        "key = random.PRNGKey(0)\n",
        "\n",
        "# Split the key to get two independent subkeys\n",
        "key, subkey1 = random.split(key) # key is updated!\n",
        "key, subkey2 = random.split(key) # key is updated again!\n",
        "\n",
        "jax_rand_set1 = random.uniform(subkey1, (3,)) # Use subkey1\n",
        "jax_rand_set2 = random.uniform(subkey2, (3,)) # Use subkey2\n",
        "\n",
        "print(f\"JAX Random Set 1:   {jax_rand_set1}\")\n",
        "print(f\"JAX Random Set 2:   {jax_rand_set2}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Random Set 1: [0.5488135  0.71518937 0.60276338]\n",
            "NumPy Random Set 2: [0.54488318 0.4236548  0.64589411]\n",
            "JAX Random Set 1:   [0.87241435 0.11105156 0.27708054]\n",
            "JAX Random Set 2:   [0.47366    0.5662228  0.88060236]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIDNiBvq-Bkc"
      },
      "source": [
        "**Key Takeaway:** JAX's explicit randomness with keys might seem a bit more verbose at first, but it's crucial for reproducibility, especially in complex ML experiments. It gives you fine-grained control over random number generation.\n",
        "\n",
        "If you want to delve deeper into JAX's random number generation, check out the resources mentioned earlier or the [official JAX documentation on randomness](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foX0uNCo-Bkc"
      },
      "source": [
        "## 2.2 Core JAX Primitives: `jit` and `grad` (10 minutes)\n",
        "\n",
        "Now, let's explore two of JAX's most powerful features (primitives): `jit` and `grad`. These are the building blocks for writing high-performance ML code in JAX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8rV76NK-Bkd"
      },
      "source": [
        "### `jit` - Speed Boost with Just-In-Time Compilation üöÄ\n",
        "\n",
        "**`jax.jit`** (Just-In-Time compilation) is your secret weapon for making JAX code run FAST. It compiles and caches your Python functions, so they execute super-efficiently using XLA, especially on accelerators like GPUs and TPUs.\n",
        "\n",
        "**How `jit` Works:**\n",
        "\n",
        "`jit` takes your Python function and transforms it into a compiled version that's optimized for your hardware. The first time you run a jitted function, JAX compiles it (this might take a bit longer initially). But subsequent calls are much faster because JAX reuses the compiled version from the cache."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1kP7KhO-Bkd",
        "outputId": "c33c93f0-6c68-4664-94dc-168217d61b89"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import time\n",
        "\n",
        "# Define a simple function\n",
        "def matmul_sum(a, b):\n",
        "    matmul =  jnp.matmul(a, b)\n",
        "    sum = jnp.sum(matmul, axis=0)\n",
        "    return sum\n",
        "\n",
        "# JIT compile the function\n",
        "fast_add = jax.jit(matmul_sum)\n",
        "\n",
        "# Example arrays\n",
        "N = 5\n",
        "x = jnp.arange(N**2).reshape(N,N)\n",
        "y = jnp.arange(N**2).reshape(N,N) + N**2\n",
        "\n",
        "# --- Timing Comparison ---\n",
        "\n",
        "print(\"--- Without JIT --- \")\n",
        "start_time = time.time()\n",
        "result_slow = matmul_sum(x, y)\n",
        "end_time = time.time()\n",
        "print(f\"Result: {result_slow}\")\n",
        "print(f\"Time taken (no jit): {end_time - start_time:.4f} seconds\\n\")\n",
        "\n",
        "print(\"--- With JIT --- \")\n",
        "result_fast = fast_add(x, y) # First call - compilation happens here\n",
        "\n",
        "start_time = time.time()\n",
        "result_fast = fast_add(x, y) # Subsequent calls are faster\n",
        "end_time = time.time()\n",
        "print(f\"Result: {result_fast}\")\n",
        "print(f\"Time taken (jit): {end_time - start_time:.4f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Without JIT --- \n",
            "Result: [10750 11050 11350 11650 11950]\n",
            "Time taken (no jit): 0.0011 seconds\n",
            "\n",
            "--- With JIT --- \n",
            "Result: [10750 11050 11350 11650 11950]\n",
            "Time taken (jit): 0.0001 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8WOY1PE-Bkd"
      },
      "source": [
        "**Observe the speed difference!** The jitted version runs much faster, especially on subsequent calls, thanks to compilation and caching.\n",
        "\n",
        "**Important Note:**  `jit` works best with **pure functions**. Pure functions are functions that:\n",
        "\n",
        "*   Always return the same output for the same inputs (no side effects).\n",
        "*   Don't rely on or modify any external state outside their scope.\n",
        "\n",
        "JAX functions are generally expected to be pure and functional for best performance and predictability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooFwh_Ju-Bkd"
      },
      "source": [
        "### `grad` - Automatic Differentiation: Your ML Superpower ‚ú®\n",
        "\n",
        "**`jax.grad`** is the automatic differentiation (autodiff) workhorse in JAX. It lets you compute gradients of Python and NumPy functions with ease. Gradients are the heart of training ML models, and JAX makes calculating them a breeze!\n",
        "\n",
        "**How `grad` Works:**\n",
        "\n",
        "`grad(f)` takes a function `f` as input and returns a *new* function that computes the gradient of `f`. If `f` is a mathematical function \\(f\\), then `grad(f)` is like its derivative \\(f'\\). Calling `grad(f)(x)` gives you the gradient of \\(f\\) evaluated at point \\(x\\)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIxsRu1S-Bkd",
        "outputId": "c0aeb295-1121-4dc2-a775-3d4bbc0798cd"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Define a function (e.g., square function)\n",
        "def square(x):\n",
        "    return x ** 2\n",
        "\n",
        "# Get the gradient function using jax.grad\n",
        "grad_square = jax.grad(square)\n",
        "\n",
        "# Evaluate the gradient at x = 3.0\n",
        "x = 3.0\n",
        "gradient_at_x = grad_square(x)\n",
        "print(f\"Gradient of square(x) at x = {x}: {gradient_at_x}\") # Should be close to 6.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of square(x) at x = 3.0: 6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jewWme0-Bkd"
      },
      "source": [
        "**Intuition:** Gradients tell you the direction of the steepest ascent of a function. In ML, we use gradients to find the direction to *minimize* our loss function (error). Gradient descent and related optimization algorithms use gradients to iteratively update model parameters and reduce loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyymCyGr-Bkd"
      },
      "source": [
        "**Key Takeaway:** `jax.grad` is incredibly powerful. It allows you to automatically compute gradients of even complex Python functions, which is essential for training neural networks without manual differentiation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tBMTGKy-Bke"
      },
      "source": [
        "## 2.3 Auto-vectorization - `vmap` (Bonus - Optional, 5 minutes)\n",
        "\n",
        "**`jax.vmap`** (Vectorizing Map) is another JAX superpower. It automatically vectorizes your functions, allowing you to efficiently apply a function to batches of data without writing explicit loops. This is crucial for performance in ML, where we often process data in batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJgkmKNp-Bke"
      },
      "source": [
        "**Example: Vectorizing the `square` function:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDadaT4P-Bke",
        "outputId": "3cb1982a-0c67-4d6c-8bc9-b1a261eab81a"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Our simple square function (operates on a single number)\n",
        "def square(x):\n",
        "    return x ** 2\n",
        "\n",
        "# Vectorize 'square' to work on batches using vmap\n",
        "vectorized_square = jax.vmap(square) # Create a batched version\n",
        "\n",
        "# Input batch of numbers\n",
        "x_batch = jnp.array([1.0, 2.0, 3.0])\n",
        "\n",
        "# Apply vectorized function to the batch\n",
        "squared_batch = vectorized_square(x_batch)\n",
        "print(f\"Squared batch: {squared_batch}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared batch: [1. 4. 9.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_IXE10B-Bke"
      },
      "source": [
        "**`vmap` Magic:** `vmap(square)` takes the `square` function (which works on single numbers) and automatically creates `vectorized_square`, which can now process entire batches (arrays) of numbers efficiently.\n",
        "\n",
        "**Benefit:** Without `vmap`, you'd have to write explicit loops to process batches, which is slower and less concise. `vmap` does the looping (vectorization) for you under the hood, making your code cleaner and faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjIPcwsZ-Bke"
      },
      "source": [
        "**Key Takeaway:** `jax.vmap` is your friend for efficient batch processing in JAX. It eliminates the need for manual loops and leverages JAX's performance optimizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eifgf7oA-Bke"
      },
      "source": [
        "### üéâ Congratulations! You've Mastered JAX Basics!\n",
        "\n",
        "You've now got a solid foundation in JAX, covering:\n",
        "\n",
        "*   NumPy Compatibility\n",
        "*   Immutability (and functional updates)\n",
        "*   Explicit Randomness with Keys\n",
        "*   `jit` for Speed\n",
        "*   `grad` for Automatic Differentiation\n",
        "*   `vmap` for Auto-vectorization (Bonus)\n",
        "\n",
        "With these JAX primitives in your toolkit, you're well-prepared to tackle more advanced ML tasks in the following practicals. Let's move on to Part 3 to build and train your first neural network!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mu23WSF-Lkb_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XqnP9-8ElBD"
      },
      "source": [
        "## 2.4 **Advanced:** Building and Training a Neural Network in JAX with Flax (20 minutes) ‚è∞\n",
        "\n",
        "Let's put our JAX knowledge to work and build a simple neural network for classification! We'll use Flax, a popular neural network library for JAX, to make things easier. We'll train this network to classify handwritten digits from the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# download Mnist and store it at ~/scikit_learn_data for offline use\n",
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
      ],
      "metadata": {
        "id": "pe6TWw6hFeXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umu5VgnbElBF"
      },
      "source": [
        "### üß± Define the Neural Network Architecture with Flax Linen **[exercise]**\n",
        "\n",
        "We'll create a simple feedforward neural network with two hidden layers using Flax Linen's `nn.Module`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcFEekmBElBF"
      },
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "    num_classes: int # Output size - number of classes for classification\n",
        "\n",
        "    @nn.compact # Linen decorator to indicate compact module definition\n",
        "    def __call__(self, x): # Flax modules are classes with a __call__ method\n",
        "        # Flatten the input image to a vector (784 features)\n",
        "        x = x.reshape((x.shape[0], -1)) # [batch_size, 28*28]\n",
        "\n",
        "        # Hidden layer 1: Dense (linear) layer followed by ReLU activation\n",
        "        x = nn.Dense(features=128)(x) # Linear transformation\n",
        "        x = nn.relu(x) # Apply ReLU activation function\n",
        "\n",
        "        # Hidden layer 2: Another Dense layer with ReLU\n",
        "        x = # ... your code here\n",
        "        x = # ... your code here\n",
        "\n",
        "        # Output layer: Dense layer to produce logits for each class\n",
        "        x = nn.Dense(features=self.num_classes)(x) # Linear layer, no activation\n",
        "        return x # Output logits (unnormalized probabilities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dtiehkzElBG"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "*   **`class SimpleClassifier(nn.Module):`**: We define our neural network as a class inheriting from `flax.linen.Module`.\n",
        "*   **`num_classes: int`**: This line defines a \"parameter\" for our module, specifying the number of output classes (10 for MNIST digits).\n",
        "*   **`@nn.compact`**: This decorator tells Flax Linen to optimize the module definition for conciseness.\n",
        "*   **`def __call__(self, x):`**: The `__call__` method defines the forward pass of our network. It takes the input `x` (an image batch) and processes it through the layers.\n",
        "*   **`x = x.reshape(...)`**: We flatten the 28x28 images into vectors of size 784, suitable as input to dense layers.\n",
        "*   **`nn.Dense(features=...)`**: These are fully connected (dense) layers. Each `nn.Dense` layer performs a linear transformation (matrix multiplication and bias addition).\n",
        "*   **`nn.relu(x)`**:  ReLU (Rectified Linear Unit) activation function is applied after each hidden dense layer to introduce non-linearity.\n",
        "*   **Output Layer:** The final `nn.Dense(features=self.num_classes)` layer produces the output logits. **No activation function is applied here** because we want the raw logits as input to our loss function (cross-entropy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGYubENsElBG"
      },
      "source": [
        "### ‚öôÔ∏è Initialize Parameters\n",
        "\n",
        "Before training, we need to initialize our model's parameters (weights and biases). We'll use Flax's `model.init` to do this.\n",
        "\n",
        "Note `model.init` takes an example input to figure out the internal shapes and computational graph of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGTBpMh3ElBG"
      },
      "source": [
        "input_shape = (-1, 28, 28, 1) # Shape of MNIST images (batch_size, height, width, channels)\n",
        "num_classes = 10 # MNIST digits 0-9\n",
        "model = SimpleClassifier(num_classes=num_classes) # Create an instance of our model\n",
        "\n",
        "key = jax.random.PRNGKey(0) # Random key for initialization\n",
        "dummy_input = jnp.ones(input_shape) # Dummy input with the correct shape\n",
        "\n",
        "# Initialize parameters (variables in Flax)\n",
        "initial_params = model.init(key, dummy_input) # Get initial parameter values\n",
        "\n",
        "print(initial_params) # Print the initialized parameters (nested dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNgHa8JeElBG"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "*   **`input_shape = (-1, 28, 28, 1)`**: We define the expected shape of our input images. The `-1` indicates a batch dimension (which can be of any size), followed by image dimensions (28x28 pixels) and channels (1 for grayscale).\n",
        "*   **`model = SimpleClassifier(num_classes=num_classes)`**: We create an instance of our `SimpleClassifier` model, telling it we have 10 output classes.\n",
        "*   **`key = jax.random.PRNGKey(0)`**: We create a random key for parameter initialization.\n",
        "*   **`dummy_input = jnp.ones(input_shape)`**: We create a dummy input tensor with the correct shape. JAX needs this to infer the shapes of the weights and biases in our network.\n",
        "*   **`initial_params = model.init(key, dummy_input)`**: This is where the magic happens! `model.init` uses the random key and dummy input to initialize the parameters of our network. The result, `initial_params`, is a nested dictionary containing the weights and biases for each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc724zufElBG"
      },
      "source": [
        "### üîÆ Define the Forward Pass (Prediction) Function  **[exercise]**\n",
        "\n",
        "Now, let's create a function for making predictions with our model. This function will take the parameters and input data and run the forward pass through the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CQMthwZElBG"
      },
      "source": [
        "# Define the forward pass function\n",
        "def forward_pass(params, x):\n",
        "    logits = model.apply(params, x) # Apply the model, get logits\n",
        "    # Convert logits to probabilities using softmax.\n",
        "    # Hint: use nn.softmax, pay attention to which axis to compute softmax over\n",
        "    probs = # ... your code here\n",
        "    return probs # Return probability distribution over classes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsqsbwKoElBG"
      },
      "source": [
        "#### Explanation: (try not to peek)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   **`def forward_pass(params, x):`**: This function takes the model parameters (`params`) and input data (`x`) as arguments.\n",
        "*   **`logits = model.apply(params, x)`**: We use `model.apply` to perform the forward pass through our network, using the provided parameters and input `x`. This gives us the raw output of the network, called \"logits\".\n",
        "*   **`probs = nn.softmax(logits, axis=-1)`**: We apply the softmax function to the logits to convert them into probabilities. Softmax ensures that the outputs sum to 1 and represent a valid probability distribution across the classes. `axis=-1` applies softmax along the class dimension.\n"
      ],
      "metadata": {
        "id": "Iipq543lHGn6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwLB7mwgElBH"
      },
      "source": [
        "### üìâ Define the Loss Function\n",
        "\n",
        "We'll use the cross-entropy loss, which is commonly used for multi-class classification. We'll also define a function to calculate accuracy, which is a more interpretable metric for evaluating our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8awM1ONnElBH"
      },
      "source": [
        "# Loss function (Cross-entropy loss for classification)\n",
        "def cross_entropy_loss(params, x_batch, y_batch):\n",
        "    probs = forward_pass(params, x_batch) # Get predictions (probabilities)\n",
        "    log_probs = jnp.log(probs) # Take the logarithm of probabilities\n",
        "    one_hot_targets = jax.nn.one_hot(y_batch, num_classes=num_classes) # One-hot encode targets\n",
        "    return -jnp.mean(jnp.sum(one_hot_targets * log_probs, axis=-1)) # Cross-entropy loss\n",
        "\n",
        "# Accuracy metric\n",
        "def accuracy(params, x_batch, y_batch):\n",
        "    preds = forward_pass(params, x_batch) # Get predictions\n",
        "    predicted_classes = jnp.argmax(preds, axis=-1) # Get class with highest probability\n",
        "    return jnp.mean(predicted_classes == y_batch) # Calculate accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnqi9h6-ElBH"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "*   **`cross_entropy_loss(params, x_batch, y_batch)`**: This function calculates the cross-entropy loss for a batch of data.\n",
        "    *   It uses `forward_pass` to get the probability predictions from our model.\n",
        "    *   It converts the integer labels `y_batch` into one-hot encoded vectors using `jax.nn.one_hot`.\n",
        "    *   It computes the cross-entropy loss using standard formulas and returns the *mean* loss over the batch.\n",
        "*   **`accuracy(params, x_batch, y_batch)`**: This function calculates the accuracy of our model on a batch of data.\n",
        "    *   It uses `forward_pass` to get predictions.\n",
        "    *   `jnp.argmax(preds, axis=-1)`:  Finds the class with the highest probability for each sample in the batch (giving us the predicted class labels).\n",
        "    *   `jnp.mean(predicted_classes == y_batch)`:  Compares the predicted classes to the true labels (`y_batch`) and calculates the mean accuracy (fraction of correct predictions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE95-YDPElBH"
      },
      "source": [
        "### üèãÔ∏è‚Äç‚ôÄÔ∏è Define the Training Loop (using Gradient Descent)\n",
        "\n",
        "Now we'll write the training loop using gradient descent to optimize our model parameters.\n",
        "\n",
        "We'll use `jax.value_and_grad`, which is similar to `jax.grad` but returns the value of the function together with the grad, and `jax.jit` to speed up the training step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E_MpXADElBH"
      },
      "source": [
        "def train_step(params, x_batch, y_batch, learning_rate):\n",
        "    \"\"\"Performs one training step (parameter update).\"\"\"\n",
        "    loss_val, grads = jax.value_and_grad(cross_entropy_loss)(params, x_batch, y_batch) # Loss and gradients\n",
        "    params_updated = jax.tree_map( # Update parameters using gradient descent\n",
        "        lambda p, g: p - learning_rate * g, params, grads\n",
        "    )\n",
        "    return loss_val, params_updated # Return updated parameters & loss value\n",
        "\n",
        "# JIT-compile the train_step for speed!\n",
        "train_step_jitted = jax.jit(train_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-NI1gGvElBH"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "*   **`def train_step(params, x_batch, y_batch, learning_rate):`**: This function defines a single training step.\n",
        "    *   **`loss_val, grads = jax.value_and_grad(...)`**:  We use `jax.value_and_grad(cross_entropy_loss)` to efficiently compute both the loss value and the gradients of the loss function *with respect to the parameters*.\n",
        "    *   **`params_updated = jax.tree_map(...)`**: We update the model parameters using gradient descent:  `new_params = old_params - learning_rate * gradients`. `jax.tree_map` is used to apply this update to each parameter in our nested parameter dictionary.\n",
        "    *   The function returns the loss value and the updated parameters.\n",
        "*   **`train_step_jitted = jax.jit(train_step)`**: We JIT-compile the `train_step` function using `jax.jit`. This will significantly speed up our training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAOOqnyOElBH"
      },
      "source": [
        "### üöÑ Run the Training Loop\n",
        "\n",
        "Now we have all the pieces to train our neural network! Let's put it all together in a training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBcxVSYhElBH"
      },
      "source": [
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "\n",
        "params = initial_params # Start with initial parameters\n",
        "train_losses = [] # To keep track of training loss over epochs\n",
        "val_accuracies = [] # To keep track of validation accuracy over epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time() # Track epoch time\n",
        "    num_train_batches = X_train.shape[0] // batch_size\n",
        "    epoch_loss = [] # Store loss for each batch in an epoch\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx in range(num_train_batches):\n",
        "        batch_start = batch_idx * batch_size\n",
        "        batch_end = batch_start + batch_size\n",
        "        x_batch = X_train[batch_start:batch_end]\n",
        "        y_batch = y_train[batch_start:batch_end]\n",
        "\n",
        "        loss_val, params = train_step_jitted( # Call JIT-compiled train step\n",
        "            params, x_batch, y_batch, learning_rate\n",
        "        )\n",
        "        epoch_loss.append(loss_val) # Store batch loss\n",
        "\n",
        "    # Calculate average loss over batches for the epoch\n",
        "    train_loss_epoch = jnp.mean(jnp.array(epoch_loss))\n",
        "    train_losses.append(train_loss_epoch) # Store epoch loss\n",
        "\n",
        "    # Calculate validation accuracy at the end of each epoch\n",
        "    val_accuracy = accuracy(params, X_test, y_test)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    epoch_time = time.time() - start_time # Epoch duration\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {train_loss_epoch:.4f} | Val Acc: {val_accuracy:.4f} | Time: {epoch_time:.2f}s\")\n",
        "\n",
        "# --- Plotting --- #\n",
        "\n",
        "# Plotting training loss curve\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(train_losses)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plotting validation accuracy curve\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(val_accuracies)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracy Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoLc1aUqElBH"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "*   **Hyperparameters:** We set hyperparameters like `num_epochs`, `batch_size`, and `learning_rate`.\n",
        "*   **Initialization:** We start with our `initial_params`.\n",
        "*   **Epoch Loop:** The outer loop iterates through the training epochs.\n",
        "    *   **Batch Iteration:** The inner loop iterates through the training data in batches (mini-batch gradient descent).\n",
        "    *   **`train_step_jitted(...)`**: We call our JIT-compiled `train_step` function to compute the loss and gradients and update parameters for each batch.\n",
        "    *   **Loss and Accuracy Tracking:** We keep track of the training loss and validation accuracy after each epoch for plotting.\n",
        "*   **Plotting:** After training, we plot the training loss and validation accuracy curves to visualize the training progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIm-OI5PElBH"
      },
      "source": [
        "### üéØ Congrats! You've Trained a Neural Network in JAX!\n",
        "\n",
        "You've successfully built and trained a simple neural network for multi-class classification using JAX and Flax! This notebook covered a lot of ground:\n",
        "\n",
        "*   Understanding JAX's core primitives (`jit`, `grad`, `vmap`)\n",
        "*   Building a neural network with Flax Linen\n",
        "*   Implementing loss and accuracy metrics\n",
        "*   Writing a training loop with gradient descent\n",
        "\n",
        "This is a significant step in your JAX and ML journey. In the next practicals, we'll build upon these fundamentals and explore more advanced deep learning models and techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zUtItEC1TMa0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NewSection-Optax"
      },
      "source": [
        "## 2.5 **[Extra] Introducing Optax: Gradient Processing and Optimization in JAX** (10 minutes) ‚è∞\n",
        "\n",
        "So far, we've used basic gradient descent to update our model parameters. But in modern machine learning, we often need more sophisticated optimization algorithms like Adam or Momentum to train models effectively and efficiently. Enter **Optax**, a powerful library built for JAX that provides gradient processing and optimization tools!\n",
        "\n",
        "**What is Optax?**\n",
        "\n",
        "Optax is a gradient processing and optimization library designed to work seamlessly with JAX. It offers:\n",
        "\n",
        "*   üõ†Ô∏è **Flexible Optimizers:** A variety of optimization algorithms like SGD (Stochastic Gradient Descent), Adam, RMSprop, and more, with customizable parameters.\n",
        "*   üîÑ **Gradient Transformations:** Tools to manipulate gradients (e.g., clipping, scaling) before applying updates.\n",
        "*   üåü **Composability:** The ability to chain multiple transformations together (e.g., combining gradient clipping with an optimizer).\n",
        "*   ‚ö° **JAX Compatibility:** Fully integrates with JAX's functional programming model and JIT compilation for high performance.\n",
        "\n",
        "In this section, we'll modify our training loop to use Optax's Adam optimizer instead of plain gradient descent, giving our neural network a boost in training efficiency!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Optax-Why"
      },
      "source": [
        "### Why Use Optax?\n",
        "\n",
        "While our basic gradient descent implementation works, it has limitations:\n",
        "\n",
        "*   **Fixed Step Size:** Gradient descent uses a constant learning rate, which can lead to slow convergence or overshooting.\n",
        "*   **No Momentum:** It doesn‚Äôt account for the history of gradients, potentially causing inefficient updates.\n",
        "\n",
        "Optax optimizers like Adam adapt the learning rate for each parameter based on past gradients, often leading to faster and more stable training. Let‚Äôs see how to integrate Optax into our workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Optax-Install"
      },
      "source": [
        "### Installing Optax\n",
        "\n",
        "First, ensure Optax is installed in your Colab environment. Run the following cell to install it if needed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Optax-Install-Code"
      },
      "source": [
        "!pip install optax\n",
        "import optax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Optax-Example"
      },
      "source": [
        "### Using Optax with Our Neural Network\n",
        "\n",
        "Let‚Äôs update our training step to use Optax‚Äôs Adam optimizer instead of basic gradient descent. We'll integrate it into the `train_step` function we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Optax-Train-Step"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "# Define the updated training step with Optax\n",
        "def train_step_optax(params, opt_state, x_batch, y_batch, optimizer):\n",
        "    \"\"\"Performs one training step using an Optax optimizer.\n",
        "\n",
        "    Args:\n",
        "        params: Model parameters.\n",
        "        opt_state: Optimizer state (e.g., momentum, moving averages).\n",
        "        x_batch: Input batch.\n",
        "        y_batch: Target batch.\n",
        "        optimizer: Optax optimizer instance.\n",
        "\n",
        "    Returns:\n",
        "        loss_val: Loss value for the batch.\n",
        "        params_updated: Updated model parameters.\n",
        "        opt_state_updated: Updated optimizer state.\n",
        "    \"\"\"\n",
        "    # Compute loss and gradients\n",
        "    loss_val, grads = jax.value_and_grad(cross_entropy_loss)(params, x_batch, y_batch)\n",
        "    # Compute parameter updates using the optimizer\n",
        "    updates, opt_state_updated = optimizer.update(grads, opt_state, params)\n",
        "    # Apply updates to parameters\n",
        "    params_updated = optax.apply_updates(params, updates)\n",
        "    return loss_val, params_updated, opt_state_updated\n",
        "\n",
        "# JIT-compile the training step for speed\n",
        "train_step_optax_jitted = jax.jit(train_step_optax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Optax-Train-Step-Explain"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "*   **`train_step_optax`**: This updated function now takes an `opt_state` (optimizer state) and an `optimizer` instance as arguments, in addition to the previous inputs.\n",
        "    *   **`loss_val, grads = jax.value_and_grad(...)`**: Same as before, we compute the loss and gradients of the loss with respect to the parameters.\n",
        "    *   **`updates, opt_state_updated = optimizer.update(...)`**: The Optax optimizer computes the updates to the parameters based on the gradients and its internal state (e.g., moving averages in Adam). It also returns the updated optimizer state.\n",
        "    *   **`params_updated = optax.apply_updates(...)`**: Apply the computed updates to the parameters to get the new parameter values.\n",
        "    *   **Returns**: We return the loss, updated parameters, and updated optimizer state.\n",
        "*   **`train_step_optax_jitted = jax.jit(...)`**: We JIT-compile this function for performance gains, just like before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Optax-Train-Loop"
      },
      "source": [
        "### Update the Training Loop with Optax\n",
        "\n",
        "Now let's modify our training loop to use the Optax-based training step. We'll initialize an Adam optimizer and manage its state during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Optax-Train-Loop-Code"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "learning_rate = 0.001  # Adjusted for Adam (typically smaller than basic GD)\n",
        "\n",
        "# Initialize the optimizer (Adam)\n",
        "optimizer = optax.adam(learning_rate=learning_rate)\n",
        "opt_state = optimizer.init(initial_params)  # Initialize optimizer state with initial parameters\n",
        "\n",
        "# Training loop with Optax\n",
        "params = initial_params  # Start with initial parameters\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    num_train_batches = X_train.shape[0] // batch_size\n",
        "    epoch_loss = []\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx in range(num_train_batches):\n",
        "        batch_start = batch_idx * batch_size\n",
        "        batch_end = batch_start + batch_size\n",
        "        x_batch = X_train[batch_start:batch_end]\n",
        "        y_batch = y_train[batch_start:batch_end]\n",
        "\n",
        "        # Perform training step with Optax\n",
        "        loss_val, params, opt_state = train_step_optax_jitted(\n",
        "            params, opt_state, x_batch, y_batch, optimizer\n",
        "        )\n",
        "        epoch_loss.append(loss_val)\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    train_loss_epoch = jnp.mean(jnp.array(epoch_loss))\n",
        "    train_losses.append(train_loss_epoch)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = accuracy(params, X_test, y_test)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {train_loss_epoch:.4f} | Val Acc: {val_accuracy:.4f} | Time: {epoch_time:.2f}s\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(train_losses)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss Curve with Optax (Adam)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot validation accuracy\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(val_accuracies)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracy Curve with Optax (Adam)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Optax-Train-Loop-Explain"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "*   **`optimizer = optax.adam(learning_rate=learning_rate)`**: We create an Adam optimizer with a specified learning rate (typically smaller than basic gradient descent, e.g., 0.001).\n",
        "*   **`opt_state = optimizer.init(initial_params)`**: Initialize the optimizer‚Äôs state (e.g., first and second moment estimates for Adam) based on the initial parameters.\n",
        "*   **Training Loop Changes**:\n",
        "    *   We now pass `opt_state` and `optimizer` to `train_step_optax_jitted`.\n",
        "    *   The function returns the updated `opt_state`, which we reuse in the next iteration to maintain the optimizer‚Äôs internal statistics (e.g., momentum).\n",
        "*   **Performance**: Adam often converges faster and more reliably than basic gradient descent, especially for neural networks, due to its adaptive learning rates.\n",
        "\n",
        "**Key Differences from Basic GD:**\n",
        "\n",
        "- Instead of manually subtracting `learning_rate * grads` from parameters, Optax handles the update logic, incorporating momentum and adaptive learning rates.\n",
        "- We manage an additional `opt_state` object, which tracks optimizer-specific variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Optax-Benefits"
      },
      "source": [
        "### Benefits of Optax\n",
        "\n",
        "*   **Improved Convergence:** Adaptive optimizers like Adam typically converge faster and more stably than basic gradient descent.\n",
        "*   **Ease of Use:** Switching to a different optimizer (e.g., RMSprop, SGD with momentum) is as simple as changing `optax.adam` to another Optax function.\n",
        "*   **Customization:** You can chain transformations like gradient clipping (`optax.clip`) or learning rate schedules with minimal code changes.\n",
        "\n",
        "**Try It Out!** Experiment with different Optax optimizers (e.g., `optax.sgd`, `optax.rmsprop`) or tweak the learning rate to see how they affect training performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Optax-Conclusion"
      },
      "source": [
        "### üöÄ Wrapping Up\n",
        "\n",
        "With Optax, you‚Äôve upgraded your JAX training pipeline to use state-of-the-art optimization techniques! This section introduced:\n",
        "\n",
        "*   The motivation behind Optax and its role in JAX.\n",
        "*   How to integrate an Optax optimizer into a neural network training loop.\n",
        "*   The practical benefits of using adaptive optimizers over basic gradient descent.\n",
        "\n",
        "You‚Äôre now equipped to explore more advanced optimization strategies in your ML experiments with JAX and Optax!"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "scrolled",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n33h9NE9-BkY"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": true,
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
