{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e06960d",
      "metadata": {
        "id": "8e06960d"
      },
      "source": [
        "# **Let's map Chile!** Intro to Geospatial Machine Learning\n",
        "\n",
        "<img src=\"https://incendios2023.dataobservatory.net/images/full.webp\" width=\"100%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/khipu-ai/practicals-2025/blob/main/notebooks/Geospatial_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "¬©Data Observatory 2025. Apache License 2.0.\n",
        "\n",
        "**Author:** [√Ålvaro Paredes L.](https://www.linkedin.com/in/alvaro-paredes-l/) (adapted from the original [Indaba Practical work](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/geospatial_machine_learning.ipynb) by [Akram Zaytar](https://www.linkedin.com/in/akramz/), [Gilles Q. Hacheme](https://www.linkedin.com/in/gilles-q-hacheme-a0956ab7/), [Aisha Alaagib](https://www.linkedin.com/in/aishaalaagib/), [Girmaw A. Tadesse](https://www.linkedin.com/in/girmaw-abebe-tadesse/).)\n",
        "\n",
        "**Reviewers:** [Mauricio Caroca](https://www.linkedin.com/in/maucaroca/), [Alejandro Antilao](https://www.linkedin.com/in/alejandro-antilao-ortiz-30a905153/)\n",
        "\n",
        "**Github link:**\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "In this notebook, we will introduce the field of geospatial machine learning by first going over the geospatial data primitives then solving a machine learning problem in an \"end-to-end\" fashion.\n",
        "\n",
        "We aim to cover the following:\n",
        "1. **Introduction to geospatial data**: vector and raster data primitives.\n",
        "2. **Problem Scoping**: introducing the problem that we are going to solve.\n",
        "3. **Approach 1**: Tabular Learning with `LightGBM`.\n",
        "    - **Data acquisition and preprocessing**: we will get the data and preprocess it for machine learning.\n",
        "    - **Model fitting**: we will fit a model to the data and conduct hyperparameter search.\n",
        "    - **Model evaluation**: we will evaluate the model on the test data.\n",
        "    - **Inference**: we will predict the output for the test data.\n",
        "4. **Approach 2**: Deep Learning with a `Sequence-to-One` model.\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: <font color='blue'>`Geospatial Data Analysis`</font>, <font color='blue'>`Computer Vision`</font>, <font color='blue'>`Tabular Data`</font>.\n",
        "\n",
        "Level: <font color='grey'>`Beginner`</font>, <font color='grey'>`Intermediate`</font>\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Learn the basics of Geospatial: differentiating between the primary data types, such as vector and raster primitives. This knowledge will form the foundation for any geospatial analysis or modeling task.\n",
        "- Comprehensive Knowledge on Geospatial Machine Learning Workflow: learn how to frame a geospatial problem, acquire and preprocess relevant data, and fit a model.\n",
        "- Diversity in Modeling Approaches: By studying two distinct approaches - tabular learning with LightGBM and deep learning using a Sequence-to-One model you will appreciate the versatility of tools and techniques available in the geospatial machine learning domain, allowing to select the best approach for different types of problems.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "Basic Machine Learning Concepts, Python Programming, Familiarity with Deep Learning, Hands-on Experience with Data Preprocessing.\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical (specially for the second part), you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "464e1ba2",
      "metadata": {
        "id": "464e1ba2"
      },
      "source": [
        "**Suggested experience level in this topic:**\n",
        "\n",
        "| Level         | Experience                            |\n",
        "| --- | --- |\n",
        "`Beginner`      | It is my first time being introduced to this work. |\n",
        "`Intermediate`  | I have done some basic courses/intros on this topic. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b3c36b1",
      "metadata": {
        "cellView": "form",
        "id": "7b3c36b1"
      },
      "outputs": [],
      "source": [
        "# @title **Paths to follow:** What is your level of experience in the topics presented in this notebook? (Run Cell)\n",
        "experience = \"advanced\"  # @param [\"beginner\", \"intermediate\", \"advanced\"]\n",
        "\n",
        "sections_to_follow = \"\"\n",
        "\n",
        "if experience == \"beginner\":\n",
        "    sections_to_follow = \"1. Introduction to Geospatial Data \\n 2.1. Problem Scoping \\n 2.2. Tabular ML with LightGBM \\n Conclusion \\n Feedback\"\n",
        "elif experience == \"intermediate\":\n",
        "    sections_to_follow = \"1. Introduction to Geospatial Data \\n 2.1. Problem Scoping \\n 2.2. Tabular ML with LightGBM \\n 2.3. Deep Learning \\n Conclusion \\n Feedback\"\n",
        "else:\n",
        "    sections_to_follow = \"1. Introduction to Geospatial Data \\n 2.1. Problem Scoping \\n 2.2. Tabular ML with LightGBM \\n 2.3. Deep Learning \\n Conclusion \\n Feedback\"\n",
        "\n",
        "print(\n",
        "    f\"Based on your experience, it is advised you follow these: \\n {sections_to_follow} sections. \\n\\x1b[3mNote this is just a guideline.\\x1b[0m\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6395e050",
      "metadata": {
        "id": "6395e050"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e21bd8e",
      "metadata": {
        "cellView": "form",
        "id": "3e21bd8e"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "%pip install rioxarray -q\n",
        "%pip install shap -q\n",
        "%pip install contextily -q\n",
        "%pip install torchgeo -q\n",
        "%pip install dask[dataframe] -q\n",
        "%pip install gdown -q\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from random import shuffle\n",
        "from typing import Any, Callable, Dict, List, Optional\n",
        "\n",
        "import gdown\n",
        "import geopandas as gpd\n",
        "import kornia.augmentation as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from lightning import LightningDataModule, LightningModule\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "from shapely.geometry import LineString, MultiLineString, MultiPolygon, Point, Polygon\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab5d715",
      "metadata": {
        "cellView": "form",
        "id": "5ab5d715"
      },
      "outputs": [],
      "source": [
        "# @hidden_cell# Set the URLs of the files\n",
        "# @title Next, we need to download the necessary files to be used in this practical:. (Run Cell)\n",
        "raster_id = \"1PfdMB3kSNjEnstRGU9_y1EwNvJykwtie\"\n",
        "landc_id = \"166fuWzs5dBcaYMoncb4yvLkdy4vi-a2B\"\n",
        "sample_id = \"1fM_9tIX_zvjBMX1yQNQ0Gjo-JFYOqRDZ\"\n",
        "\n",
        "raster_url = f\"https://drive.google.com/uc?id={raster_id}\"\n",
        "landc_url = f\"https://drive.google.com/uc?id={landc_id}\"\n",
        "sample_url = f\"https://drive.google.com/uc?id={sample_id}\"\n",
        "\n",
        "# Set the path of the directory\n",
        "data_dir = Path(\"./files\")\n",
        "if data_dir.exists():\n",
        "    shutil.rmtree(data_dir, ignore_errors=True)\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Download files\n",
        "gdown.download(raster_url, str(data_dir / \"chile_coverage_2018.tif\"), quiet=False)\n",
        "gdown.download(landc_url, str(data_dir / \"df.parquet\"), quiet=False)\n",
        "gdown.download(sample_url, str(data_dir / \"wetlands_2015.parquet\"), quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f20b000",
      "metadata": {
        "id": "4f20b000"
      },
      "source": [
        "# 1. Introduction to Geospatial Data üåé"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12463c32",
      "metadata": {
        "id": "12463c32"
      },
      "source": [
        "Have you ever wondered how we can analyze satellite images of farms, forests, cities, water bodies, etc.?\n",
        "Or how we determine crop health from space? Let's dive into the exciting world of **geospatial data**!\n",
        "\n",
        "Geospatial data is basically **digital information about Earth's surface**. It's what powers:\n",
        "- üó∫Ô∏è Your favorite maps app\n",
        "- üõ∞Ô∏è Climate monitoring systems\n",
        "- üöú Precision agriculture\n",
        "- üåæ Crop yield prediction\n",
        "\n",
        "We'll explore two main ways to work with geospatial data:\n",
        "\n",
        "<div style=\"text-align:center;\"> <figure> <img width=\"500px\" src=\"https://i0.wp.com/pangeography.com/wp-content/uploads/2022/05/Raster_vector_tikz.png\" /> <figcaption style=\"font-size:small;\">Image credit: <a href=\"https://pangeography.com/geographic-data-structure-vector-data-and-raster-data/\">Pan Geography</a></figcaption> </figure> </div>\n",
        "\n",
        "1. **Vector Data** üìç: Think of drawing shapes on a map (points, lines, polygons)\n",
        "2. **Raster Data** üñºÔ∏è: Think of satellite photos divided into pixels\n",
        "\n",
        "## What are the main uses?\n",
        "\n",
        "### **Vector Data Applications** üìç\n",
        "1. **Urban Planning**: Mapping city infrastructure (roads, buildings) for smart city development [^1](https://www.esri.com/en-us/industries/urban-planning/overview)\n",
        "2. **Navigation**: GPS systems use vector data for route optimization and real-time tracking [^2](https://www.tomtom.com/products/navigation/)\n",
        "3. **Utility Management**: Mapping power lines, water pipes, and gas networks for maintenance [^3](https://www.geospatialworld.net/blogs/utility-mapping-using-gis/)\n",
        "4. **E-commerce**: Delivery route optimization and service area mapping [^4](https://www.flexport.com/blog/how-geospatial-data-is-transforming-logistics/)\n",
        "\n",
        "### **Raster Data Applications** üñºÔ∏è\n",
        "1. **Land Cover Analysis**: Monitoring vegetation changes for agriculture and forestry [^5](https://www.usgs.gov/land-resources/eros/land-cover)\n",
        "2. **Disaster Management**: Flood mapping and wildfire detection using satellite imagery [^6](https://www.un-spider.org/)\n",
        "3. **Climate Monitoring**: Tracking glacier retreat and sea level rise [^7](https://climate.nasa.gov/)\n",
        "4. **Precision Agriculture**: Crop health monitoring using NDVI from satellite data [^8](https://www.agriculture.com/technology/crop-management/precision-agriculture)\n",
        "\n",
        "### **Emerging Applications** üöÄ\n",
        "- **Autonomous Vehicles**: Real-time mapping for self-driving cars\n",
        "- **Augmented Reality**: Location-based AR experiences using geospatial data\n",
        "- **Public Health**: Disease spread modeling using population density maps\n",
        "- **Renewable Energy**: Solar potential mapping using elevation data\n",
        "\n",
        "Let's start exploring these!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0cffb9a",
      "metadata": {
        "id": "d0cffb9a"
      },
      "source": [
        "## 1.1 Drawing on Earth: Vector Data üéØ (Vector Beginner)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "543202a3",
      "metadata": {
        "id": "543202a3"
      },
      "source": [
        "Vector data represents the world using **geometric shapes**. Imagine marking:\n",
        "- The exact location of a farm (point)\n",
        "- The path of a river (line)\n",
        "- The boundary of a field (polygon)\n",
        "\n",
        "We'll use a library called `shapely` to create and manipulate these shapes, and then look at real farm data from Chile!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cca3079",
      "metadata": {
        "id": "2cca3079",
        "lines_to_next_cell": 2
      },
      "source": [
        "### Creating Points: Marking Locations üìå"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5438c13",
      "metadata": {
        "cellView": "form",
        "id": "a5438c13"
      },
      "outputs": [],
      "source": [
        "# @title Creating Points in central Chile üá®üá±\n",
        "# @hidden_cell#\n",
        "\n",
        "\n",
        "# Create a point representing a location (Santiago area)\n",
        "farm_location = Point(-70.6693, -33.4489)\n",
        "print(f\"Our forest is located at: {farm_location.x}, {farm_location.y}\")\n",
        "\n",
        "# Let's create another point (vineyard in wine country)\n",
        "vineyard = Point(-71.3, -33.1)\n",
        "print(f\"Our vineyard is located at: {vineyard.x}, {vineyard.y}\")\n",
        "\n",
        "# Simple visualization\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.scatter(\n",
        "    [farm_location.x, vineyard.x],\n",
        "    [farm_location.y, vineyard.y],\n",
        "    color=[\"red\", \"purple\"],\n",
        "    s=100,\n",
        ")\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.set_title(\"Key Agricultural Locations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e26f671e",
      "metadata": {
        "id": "e26f671e"
      },
      "source": [
        "### Creating Polygons: Drawing Field Boundaries üåæ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f38213",
      "metadata": {
        "cellView": "form",
        "id": "a4f38213"
      },
      "outputs": [],
      "source": [
        "# @title Drawing a Field\n",
        "\n",
        "# Create a polygon representing a field Central Valley\n",
        "field = Polygon(\n",
        "    [\n",
        "        (-71.5, -33.5),  # SW corner\n",
        "        (-71.5, -33.4),  # NW corner\n",
        "        (-71.4, -33.4),  # NE corner\n",
        "        (-71.4, -33.5),  # SE corner\n",
        "        (-71.5, -33.5),  # Back to start\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"Our field has an area of: {field.area:.6f} square degrees\")\n",
        "print(f\"That's approximately {field.area * 111**2:.2f} square kilometers\")\n",
        "\n",
        "# Visualize the field\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "x, y = field.exterior.xy\n",
        "ax.plot(x, y, \"o-\", color=\"green\", linewidth=2)\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.set_title(\"Agricultural Field\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dca3b3aa",
      "metadata": {
        "id": "dca3b3aa"
      },
      "source": [
        "### Creating Complex Shapes: Multiple Geometries üß©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "676ba125",
      "metadata": {
        "cellView": "form",
        "id": "676ba125",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# @title Multiple Fields and Irrigation Channels\n",
        "\n",
        "# Create multiple fields (vineyard plots)\n",
        "field1 = Polygon(\n",
        "    [(-71.5, -33.5), (-71.5, -33.4), (-71.4, -33.4), (-71.4, -33.5), (-71.5, -33.5)]\n",
        ")\n",
        "field2 = Polygon(\n",
        "    [\n",
        "        (-71.35, -33.45),\n",
        "        (-71.35, -33.35),\n",
        "        (-71.25, -33.35),\n",
        "        (-71.25, -33.45),\n",
        "        (-71.35, -33.45),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create irrigation channels (lines)\n",
        "channel1 = LineString([(-71.5, -33.45), (-71.4, -33.45)])\n",
        "channel2 = LineString([(-71.35, -33.4), (-71.25, -33.4)])\n",
        "channel3 = LineString([(-71.4, -33.45), (-71.35, -33.4)])  # Connecting channel\n",
        "\n",
        "# Group related geometries\n",
        "vineyards = MultiPolygon([field1, field2])\n",
        "irrigation = MultiLineString([channel1, channel2, channel3])\n",
        "\n",
        "# Plot our farm layout\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Plot fields\n",
        "for field in vineyards.geoms:\n",
        "    x, y = field.exterior.xy\n",
        "    ax.fill(x, y, alpha=0.5, fc=\"green\", ec=\"black\")\n",
        "\n",
        "# Plot irrigation channels\n",
        "for channel in irrigation.geoms:\n",
        "    x, y = channel.xy\n",
        "    ax.plot(x, y, \"blue\", linewidth=2)\n",
        "\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.set_title(\"Vineyard Layout with Irrigation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50080774",
      "metadata": {
        "id": "50080774"
      },
      "source": [
        "### <font color=\"red\"> **Exercise 1.1.** üéÆ Your Turn: create your own geometries!\n",
        "Can you create a field with a unique shape? Maybe add some irrigation channels or buildings?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d392ede",
      "metadata": {
        "cellView": "form",
        "id": "7d392ede"
      },
      "outputs": [],
      "source": [
        "# @title Design Your Farm\n",
        "# Create your own farm layout below!\n",
        "\n",
        "# Your field boundary (try a different shape than a rectangle)\n",
        "my_field = Polygon(\n",
        "    [\n",
        "        # Add your coordinates here!\n",
        "        # Example: (-71.5, -33.5)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Maybe add a building or water feature?\n",
        "my_building = None  # Replace with a Point or small Polygon\n",
        "\n",
        "# Visualize your creation\n",
        "# Your plotting code here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177cac9a",
      "metadata": {
        "cellView": "form",
        "id": "177cac9a"
      },
      "outputs": [],
      "source": [
        "# @title Example Solution: Triangular Vineyard with Pond\n",
        "# @hidden_cell#\n",
        "\n",
        "# Create a triangular field\n",
        "triangle_field = Polygon(\n",
        "    [\n",
        "        (-71.5, -33.5),  # South point\n",
        "        (-71.45, -33.4),  # Northeast point\n",
        "        (-71.55, -33.4),  # Northwest point\n",
        "        (-71.5, -33.5),  # Back to start\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Add a small circular pond (approximated with many points)\n",
        "from shapely.geometry import Point\n",
        "\n",
        "pond_center = Point(-71.5, -33.45)\n",
        "pond_radius = 0.01\n",
        "pond_points = []\n",
        "for angle in np.linspace(0, 2 * np.pi, 20):\n",
        "    x = pond_center.x + pond_radius * np.cos(angle)\n",
        "    y = pond_center.y + pond_radius * np.sin(angle)\n",
        "    pond_points.append((x, y))\n",
        "pond_points.append(pond_points[0])  # Close the circle\n",
        "pond = Polygon(pond_points)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "# Plot field\n",
        "field_x, field_y = triangle_field.exterior.xy\n",
        "ax.fill(field_x, field_y, alpha=0.5, fc=\"green\", ec=\"black\")\n",
        "# Plot pond\n",
        "pond_x, pond_y = pond.exterior.xy\n",
        "ax.fill(pond_x, pond_y, alpha=0.7, fc=\"blue\", ec=\"black\")\n",
        "\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.set_title(\"Triangular Vineyard with Pond\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bb5c927",
      "metadata": {
        "id": "3bb5c927"
      },
      "source": [
        "### Measuring and Analyzing Shapes üìè"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f77c97",
      "metadata": {
        "cellView": "form",
        "id": "24f77c97"
      },
      "outputs": [],
      "source": [
        "# @title Key Properties of Geometric Objects\n",
        "# @hidden_cell#\n",
        "\n",
        "# Create a vineyard field\n",
        "vineyard = Polygon(\n",
        "    [(-71.5, -33.5), (-71.5, -33.4), (-71.4, -33.4), (-71.4, -33.5), (-71.5, -33.5)]\n",
        ")\n",
        "\n",
        "# 1. Area - How large is the field?\n",
        "print(f\"Vineyard area: {vineyard.area:.6f} square degrees\")\n",
        "print(f\"Approximate area: {vineyard.area * 111**2:.2f} square kilometers\")\n",
        "\n",
        "# 2. Perimeter - How long is the fence around it?\n",
        "print(f\"Perimeter: {vineyard.length:.6f} degrees\")\n",
        "print(f\"Approximate perimeter: {vineyard.length * 111:.2f} kilometers\")\n",
        "\n",
        "# 3. Centroid - What's the center point? (useful for labels)\n",
        "center = vineyard.centroid\n",
        "print(f\"Center of the vineyard: {center.x:.6f}, {center.y:.6f}\")\n",
        "\n",
        "# 4. Bounds - What's the bounding box? (useful for zooming a map)\n",
        "bbox = vineyard.bounds  # (min_x, min_y, max_x, max_y)\n",
        "print(f\"Bounding box: {bbox}\")\n",
        "\n",
        "# Visualize these properties\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "# Plot field\n",
        "x, y = vineyard.exterior.xy\n",
        "ax.fill(x, y, alpha=0.3, fc=\"green\", ec=\"black\", label=\"Vineyard\")\n",
        "# Plot centroid\n",
        "ax.scatter(center.x, center.y, color=\"red\", s=100, label=\"Center\")\n",
        "# Plot bounding box\n",
        "min_x, min_y, max_x, max_y = bbox\n",
        "ax.plot(\n",
        "    [min_x, max_x, max_x, min_x, min_x],\n",
        "    [min_y, min_y, max_y, max_y, min_y],\n",
        "    \"r--\",\n",
        "    label=\"Bounding Box\",\n",
        ")\n",
        "\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.set_title(\"Vineyard Properties\")\n",
        "ax.legend(bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb660226",
      "metadata": {
        "id": "eb660226"
      },
      "source": [
        "#### Properties of Geometric Objects [WARNING]:\n",
        "\n",
        "In this example we're calculating the area and perimeter of a polygon using degrees coordinates. They're not specifically set (we haven't yet specified a Coordinate Reference System, also kwown as CRS), but when we do this, the area and perimeter are not what you expect, and may vary depending on the location on Earth, as one degree is not the same size everywhere.\n",
        "\n",
        "Also, distance/area are linear and we're not considering the curvature of the Earth. To really compute areas, distances and perimeters, we need to specify an appropriate CRS.\n",
        "\n",
        "Anyway, these are simple examples to understand the concept (they work with the same principles)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab3fd701",
      "metadata": {
        "id": "ab3fd701"
      },
      "source": [
        "### Field Operations: Combining and Comparing Shapes üîß [OPTIONAL]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6c47e7",
      "metadata": {
        "cellView": "form",
        "id": "bb6c47e7"
      },
      "outputs": [],
      "source": [
        "# @title Spatial Operations Between Fields\n",
        "# @hidden_cell#\n",
        "\n",
        "# Create two neighboring fields\n",
        "field1 = Polygon(\n",
        "    [(-71.5, -33.5), (-71.5, -33.4), (-71.4, -33.4), (-71.4, -33.5), (-71.5, -33.5)]\n",
        ")\n",
        "\n",
        "field2 = Polygon(\n",
        "    [\n",
        "        (-71.45, -33.45),\n",
        "        (-71.45, -33.35),\n",
        "        (-71.35, -33.35),\n",
        "        (-71.35, -33.45),\n",
        "        (-71.45, -33.45),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Get the bounding box of the fields, so all subplots have the same extent\n",
        "x_coords = []\n",
        "y_coords = []\n",
        "for geom in [field1, field2]:\n",
        "    if not geom.is_empty and geom.geom_type == \"Polygon\":\n",
        "        x, y = geom.exterior.xy\n",
        "        x_coords.extend(x)\n",
        "        y_coords.extend(y)\n",
        "\n",
        "x_min, x_max = min(x_coords), max(x_coords)\n",
        "y_min, y_max = min(y_coords), max(y_coords)\n",
        "\n",
        "# Add some padding (5% of range)\n",
        "x_pad = 0.05 * (x_max - x_min)\n",
        "y_pad = 0.05 * (y_max - y_min)\n",
        "x_limits = [x_min - x_pad, x_max + x_pad]\n",
        "y_limits = [y_min - y_pad, y_max + y_pad]\n",
        "\n",
        "# 1. Union - Combine fields (e.g., when a farmer buys neighboring land)\n",
        "combined_fields = field1.union(field2)\n",
        "print(f\"Total area of combined fields: {combined_fields.area:.6f} square degrees\")\n",
        "\n",
        "# 2. Intersection - Find overlapping areas (e.g., disputed territory)\n",
        "overlap = field1.intersection(field2)\n",
        "print(f\"Overlapping area: {overlap.area:.6f} square degrees\")\n",
        "\n",
        "# 3. Difference - Remove one area from another (e.g., sell part of your land)\n",
        "remaining_land = field1.difference(field2)\n",
        "print(\n",
        "    f\"Area after selling the overlapping part: {remaining_land.area:.6f} square degrees\"\n",
        ")\n",
        "\n",
        "# 4. Spatial relationships\n",
        "print(f\"Fields touching each other? {field1.touches(field2)}\")\n",
        "print(f\"Fields overlapping? {field1.overlaps(field2)}\")\n",
        "print(f\"One field contains the other? {field1.contains(field2)}\")\n",
        "\n",
        "# Visualize these operations\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Original fields\n",
        "ax = axs[0, 0]\n",
        "x1, y1 = field1.exterior.xy\n",
        "ax.fill(x1, y1, alpha=0.5, fc=\"green\", ec=\"black\", label=\"Field 1\")\n",
        "x2, y2 = field2.exterior.xy\n",
        "ax.fill(x2, y2, alpha=0.5, fc=\"blue\", ec=\"black\", label=\"Field 2\")\n",
        "ax.set_title(\"Original Fields\")\n",
        "ax.legend()\n",
        "\n",
        "# Union\n",
        "ax = axs[0, 1]\n",
        "x, y = combined_fields.exterior.xy\n",
        "ax.fill(x, y, alpha=0.5, fc=\"purple\", ec=\"black\")\n",
        "ax.set_title(\"Union (Combined Fields)\")\n",
        "\n",
        "# Intersection\n",
        "ax = axs[1, 0]\n",
        "if not overlap.is_empty:\n",
        "    if overlap.geom_type == \"Polygon\":\n",
        "        x, y = overlap.exterior.xy\n",
        "        ax.fill(x, y, alpha=0.5, fc=\"red\", ec=\"black\")\n",
        "ax.set_title(\"Intersection (Overlap)\")\n",
        "\n",
        "# Difference\n",
        "ax = axs[1, 1]\n",
        "if remaining_land.geom_type == \"Polygon\":\n",
        "    x, y = remaining_land.exterior.xy\n",
        "    ax.fill(x, y, alpha=0.5, fc=\"orange\", ec=\"black\")\n",
        "ax.set_title(\"Difference (Field 1 - Field 2)\")\n",
        "\n",
        "for ax in axs.flatten():\n",
        "    ax.set_xlabel(\"Longitude\")\n",
        "    ax.set_ylabel(\"Latitude\")\n",
        "    ax.set_xlim(x_limits)\n",
        "    ax.set_ylim(y_limits)\n",
        "    ax.set_aspect(\"equal\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5242ef2",
      "metadata": {
        "id": "b5242ef2"
      },
      "source": [
        "### <font color=\"red\"> **Exercise 1.2.**üå∂Ô∏è Challenge: Field Detective [OPTIONAL]\n",
        "Find out if two new fields overlap or are neighbors, and calculate the total area if combined!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f339fd6d",
      "metadata": {
        "cellView": "form",
        "id": "f339fd6d"
      },
      "outputs": [],
      "source": [
        "# @title Field Detective Challenge\n",
        "# Two farmers want to know if their fields overlap or are neighbors\n",
        "\n",
        "field_a = Polygon(\n",
        "    [(-71.5, -33.5), (-71.5, -33.4), (-71.4, -33.4), (-71.4, -33.5), (-71.5, -33.5)]\n",
        ")\n",
        "\n",
        "field_b = None  # Create your own field here!\n",
        "\n",
        "# Check spatial relationships\n",
        "touches = None  # Do they touch? (neighbors)\n",
        "overlaps = None  # Do they overlap?\n",
        "total_area = None  # What would be the combined area?\n",
        "\n",
        "# Your code here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aac26d24",
      "metadata": {
        "cellView": "form",
        "id": "aac26d24"
      },
      "outputs": [],
      "source": [
        "# @title Field Detective Solution\n",
        "# @hidden_cell#\n",
        "\n",
        "field_b = Polygon(\n",
        "    [\n",
        "        (-71.4, -33.5),  # SW - touches field_a\n",
        "        (-71.4, -33.4),  # NW - touches field_a\n",
        "        (-71.3, -33.4),  # NE\n",
        "        (-71.3, -33.5),  # SE\n",
        "        (-71.4, -33.5),  # Back to SW\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check spatial relationships\n",
        "touches = field_a.touches(field_b)\n",
        "overlaps = field_a.overlaps(field_b)\n",
        "total_area = field_a.union(field_b).area\n",
        "\n",
        "# Results\n",
        "print(f\"Fields are neighbors (touching): {touches}\")\n",
        "print(f\"Fields overlap: {overlaps}\")\n",
        "print(f\"Combined area: {total_area:.6f} square degrees\")\n",
        "print(f\"Approximate combined area: {total_area * 111**2:.2f} square kilometers\")\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "x1, y1 = field_a.exterior.xy\n",
        "ax.fill(x1, y1, alpha=0.5, fc=\"green\", ec=\"black\", label=\"Field A\")\n",
        "x2, y2 = field_b.exterior.xy\n",
        "ax.fill(x2, y2, alpha=0.5, fc=\"blue\", ec=\"black\", label=\"Field B\")\n",
        "\n",
        "# Union boundary\n",
        "union = field_a.union(field_b)\n",
        "x, y = union.exterior.xy\n",
        "ax.plot(x, y, \"r--\", linewidth=2, label=\"Combined Boundary\")\n",
        "\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.set_title(\"Neighboring Fields Analysis\")\n",
        "ax.legend(bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2442c9",
      "metadata": {
        "id": "aa2442c9"
      },
      "source": [
        "### Working with Real Data üöú"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e758acc",
      "metadata": {
        "cellView": "form",
        "id": "0e758acc"
      },
      "outputs": [],
      "source": [
        "# @title Loading Real Data with GeoPandas\n",
        "# @hidden_cell#\n",
        "\n",
        "# Load our field data\n",
        "gdf = gpd.read_parquet(data_dir / \"wetlands_2015.parquet\")\n",
        "\n",
        "# Show sample data\n",
        "print(gdf.head())\n",
        "\n",
        "# Plot sample fields\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "gdf.plot(\n",
        "    ax=ax,\n",
        "    column=\"Nombre\",\n",
        "    categorical=True,\n",
        "    legend=False,\n",
        "    markersize=100,\n",
        "    marker=\"o\",\n",
        "    cmap=\"viridis\",\n",
        ")\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.set_title(\"Wetlands polygons\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "865b2f55",
      "metadata": {
        "id": "865b2f55"
      },
      "outputs": [],
      "source": [
        "# @title <font color=\"red\"> **Exercise 1.3.** Challenge: Can you get the area (in hectares) of each wetland?\n",
        "# Beware of:\n",
        "# 1. The CRS (take a look at the axis values in the plot)\n",
        "# 2. The units of the CRS (degrees, meters, etc.)\n",
        "# 3. Should I compute the area or can I get the area from the attribute table?\n",
        "wetlands_crs = None\n",
        "wetlands_area_ha = None\n",
        "wetlands_crs_units = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "238ba695",
      "metadata": {
        "id": "238ba695",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# @title Challenge: Solution\n",
        "wetlands_crs = gdf.crs.to_epsg()  # could be also gdf.crs.name\n",
        "wetlands_area_ha = (\n",
        "    gdf.area / 10000\n",
        ")  # beware of using the column \"Ha_humedal\", as it may not be the same (clipped data)\n",
        "wetlands_crs_units = gdf.crs.axis_info[0].unit_name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e850b91c",
      "metadata": {
        "id": "e850b91c"
      },
      "source": [
        "## 1.2 Satellite Views: Raster Data üõ∞Ô∏è (Raster Beginner)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d77db5",
      "metadata": {
        "id": "93d77db5",
        "lines_to_next_cell": 2
      },
      "source": [
        "Raster data represents the world as a grid of pixels (think digital photos). Each pixel has values representing:\n",
        "- Visible colors (red, green, blue)\n",
        "- Invisible light (near-infrared, thermal)\n",
        "- Other measurements (elevation, temperature)\n",
        "\n",
        "Landsat and Sentinel-2 satellites capture multiple \"bands\" of light to help analyze:\n",
        "- Crop health\n",
        "- Soil moisture\n",
        "- Land cover\n",
        "- And much more!\n",
        "\n",
        "Let's explore the Mapbiomass raster product, which is a product of the Mapbiomas project and is calculated from Landsat data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78fca96d",
      "metadata": {
        "cellView": "form",
        "id": "78fca96d"
      },
      "outputs": [],
      "source": [
        "# @title Exploring Chilean Land Cover Data\n",
        "# @hidden_cell#\n",
        "\n",
        "import rioxarray as rxr\n",
        "\n",
        "# Load our land cover image\n",
        "landcover = rxr.open_rasterio(\"files/chile_coverage_2018.tif\")\n",
        "print(\"Successfully loaded land cover data!\")\n",
        "print(f\"Image shape: {landcover.shape}\")\n",
        "\n",
        "# Get basic metadata\n",
        "print(\"\\nImage metadata:\")\n",
        "print(f\"CRS: {landcover.rio.crs}\")\n",
        "print(f\"Resolution: {landcover.rio.resolution()}\")\n",
        "print(f\"Bounds: {landcover.rio.bounds()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e164d6c4",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "e164d6c4"
      },
      "outputs": [],
      "source": [
        "# @title Where is this data located?\n",
        "\n",
        "import contextily as ctx\n",
        "from shapely.geometry import box\n",
        "\n",
        "minx, miny, maxx, maxy = landcover.rio.bounds()\n",
        "\n",
        "# Create a GeoDataFrame with the bounding box\n",
        "bbox_geom = box(minx, miny, maxx, maxy)\n",
        "bbox_gdf = gpd.GeoDataFrame(geometry=[bbox_geom], crs=landcover.rio.crs)\n",
        "\n",
        "# If needed, reproject to WGS84 (EPSG:4326) for mapping\n",
        "if bbox_gdf.crs != \"EPSG:4326\":\n",
        "    bbox_gdf = bbox_gdf.to_crs(\"EPSG:4326\")\n",
        "    print(f\"Reprojected bounds: {bbox_gdf.total_bounds}\")\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(6, 8))\n",
        "\n",
        "# Plot the bounding box with transparent fill and red stroke\n",
        "bbox_gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"red\", linewidth=2)\n",
        "bbox_gdf.buffer(5).plot(ax=ax, facecolor=\"none\", edgecolor=\"none\", linewidth=2)\n",
        "\n",
        "# Add a basemap\n",
        "ctx.add_basemap(ax, crs=bbox_gdf.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
        "\n",
        "# Add title and labels\n",
        "ax.set_title(\"Land Cover Data Extent in Chile\", fontsize=14)\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61484596",
      "metadata": {
        "cellView": "form",
        "id": "61484596"
      },
      "outputs": [],
      "source": [
        "# @title Visualizing Land Cover Types\n",
        "# @hidden_cell#\n",
        "\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.patches as mpatches\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Get unique values from the data\n",
        "unique_values = np.unique(landcover[0].values)\n",
        "unique_values = unique_values[~np.isnan(unique_values)]  # Remove NaN if present\n",
        "\n",
        "palette = sns.color_palette(\"tab10\", len(unique_values))\n",
        "\n",
        "# Convert colors to hex format for Matplotlib\n",
        "landcover_colors = {\n",
        "    val: colors.to_hex(palette[i]) for i, val in enumerate(unique_values)\n",
        "}\n",
        "\n",
        "# Create colormap from the actual values\n",
        "landcover_cmap = colors.ListedColormap(list(landcover_colors.values()))\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "landcover.plot(\n",
        "    ax=ax,\n",
        "    cmap=landcover_cmap,\n",
        "    vmin=min(unique_values) - 0.5,\n",
        "    vmax=max(unique_values) + 0.5,\n",
        "    add_colorbar=False,\n",
        ")\n",
        "ax.set_aspect(\"equal\")  # or ax.set_aspect(1)\n",
        "\n",
        "# Add legend with actual values\n",
        "legend_patches = [\n",
        "    mpatches.Patch(color=landcover_colors[val], label=f\"Class {val}\")\n",
        "    for val in unique_values\n",
        "]\n",
        "ax.legend(\n",
        "    handles=legend_patches,\n",
        "    loc=\"center left\",\n",
        "    bbox_to_anchor=(1, 0.5),\n",
        "    title=\"Land Cover Classes\",\n",
        ")\n",
        "\n",
        "ax.set_title(\"Land Cover Classification\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21d5a79",
      "metadata": {
        "id": "f21d5a79"
      },
      "source": [
        "What means each digital value (DV) \"code\" ?\n",
        "\n",
        "We'll review them in more detail in the next section (2.1), but here's a quick overview:\n",
        "\n",
        "| Clase Nivel 2                     | DV  |\n",
        "|-----------------------------------|-----|\n",
        "| 1.1 Bosque                       | 3   |\n",
        "| 2.1 Humedal                      | 11  |\n",
        "| 2.2 Pastizal                     | 12  |\n",
        "| 2.3 Matorral                     | 66  |\n",
        "| 2.4 Afloramiento rocoso          | 29  |\n",
        "| 3.1 Plantaci√≥n Forestal          | 9   |\n",
        "| 3.2 Mosaico de agricultura y pastura | 21  |\n",
        "| 4.1 Infraestructura              | 24  |\n",
        "| 4.2 Arenas, Playas y Dunas       | 23  |\n",
        "| 4.3 Salar                        | 61  |\n",
        "| 4.4 Otra √°rea sin vegetaci√≥n     | 25  |\n",
        "| 5.1 R√≠o, lago u oc√©ano           | 33  |\n",
        "| 5.2 Hielo y nieve                | 34  |\n",
        "| No observado                     | 27  |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa253e1",
      "metadata": {
        "id": "9aa253e1"
      },
      "source": [
        "### <font color=\"red\"> **Exercise 1.4.**üé® Challenge: Land Cover Analysis!\n",
        "Can you calculate the percentage of each land cover type in the image?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a50627",
      "metadata": {
        "cellView": "form",
        "id": "47a50627"
      },
      "outputs": [],
      "source": [
        "# @title Analyze Land Cover Distribution\n",
        "# Your code here to calculate percentages of each land cover type\n",
        "# Hint: use np.unique() with return_counts=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b95aaa",
      "metadata": {
        "cellView": "form",
        "id": "49b95aaa"
      },
      "outputs": [],
      "source": [
        "# @title Example Solution: Land Cover Distribution\n",
        "# @hidden_cell#\n",
        "\n",
        "# Print class distribution\n",
        "values, counts = np.unique(landcover.values, return_counts=True)\n",
        "\n",
        "# Calculate land cover type percentages\n",
        "total_pixels = counts.sum()\n",
        "percentages = (counts / total_pixels) * 100\n",
        "\n",
        "# Plot as a pie chart\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "# Pie chart\n",
        "ax1.pie(\n",
        "    percentages,\n",
        "    labels=values,\n",
        "    colors=list(landcover_colors.values()),\n",
        "    autopct=\"%1.1f%%\",\n",
        ")\n",
        "ax1.set_title(\"Land Cover Distribution\")\n",
        "\n",
        "# Bar chart\n",
        "ax2.bar(values, percentages, color=list(landcover_colors.values()))\n",
        "ax2.set_ylabel(\"Percentage of Total Area\")\n",
        "ax2.set_title(\"Land Cover Distribution\")\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Print the percentages\n",
        "print(\"\\nLand Cover Distribution:\")\n",
        "for val, count in zip(values, counts):\n",
        "    percentage = (count / counts.sum()) * 100\n",
        "    print(f\"Class {val}: {percentage:.1f}%\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dce49af3",
      "metadata": {
        "id": "dce49af3"
      },
      "source": [
        "## What's Next? üöÄ\n",
        "Now that you understand both vector (field boundaries) and raster (satellite images) data:\n",
        "\n",
        "1. We can combine them to analyze crops in specific fields\n",
        "2. Track changes over time using multiple satellite images\n",
        "3. Apply machine learning to predict crop types and health!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d10030b",
      "metadata": {
        "id": "9d10030b"
      },
      "source": [
        "After this quick introduction to geospatial data, let's move to learning more about Geospatial ML by trying to solve a problem.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0329f91",
      "metadata": {
        "id": "d0329f91",
        "lines_to_next_cell": 2
      },
      "source": [
        "# 2. Landcover Classification in Central Chile\n",
        "\n",
        "We'll use Geospatial ML to classify land cover data based in the MAPBIOMASS project, for a part of the O'Higgins region in Chile.\n",
        "\n",
        "<div style=\"text-align:center;\">\n",
        "    <figure>\n",
        "        <img style=\"width:50%;\" src=\"https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2022/11/santiago_chile/24566117-2-eng-GB/Santiago_Chile.jpg\" />\n",
        "        <figcaption style=\"font-size:small;\">Question: <b>What season is it?</b> (Image credit: <a href=\"https://www.esa.int/\">ESA</a>)</figcaption>\n",
        "    </figure>\n",
        "</div>\n",
        "\n",
        "### How was the data generated?\n",
        "\n",
        "Is a long-short story. Chile doesn't have open data for landcover or agriculture. The available data is very aggregated and not very useful for our purposes.\n",
        "\n",
        "So, we decided to generate our own data by sampling the landcover map at random points, but not so random. We wanted to sample points that are representative of the landcover in the region, and this means include some landcover types that are less frequent.\n",
        "\n",
        "We follow these steps in order to generate data for this practical (Machine Learning and Deep Learning Challenge):\n",
        "\n",
        "1. We selected a region of Chile that has a mix of landcover types.\n",
        "2. That region was polygonized.\n",
        "3. The area of each polygon (landcover type) was calculated.\n",
        "\n",
        "Then, for the ML part:\n",
        "\n",
        "1. Filter farms (predio_id) by minimum area (10 hectares) and specific land use classes (`desired_classes`).\n",
        "2. Perform stratified sampling to balance the number of farms and total area per land use class.\n",
        "3. Use a weighted sampling approach based on farm area and class distribution.\n",
        "4. Ensure the final sample size matches the target (`sample_size = 3000`).\n",
        "5. Reproject the sampled farms to UTM Zone 19S (EPSG:32719) for accurate spatial operations.\n",
        "6. Create a grid of points within each farm polygon using a 10-meter resolution.\n",
        "7. Ensure points are centered within grid cells.\n",
        "8. Spatial join the grid points with the original farm polygons to associate each point with a `predio_id` and land use class (`value`).\n",
        "\n",
        "\n",
        "For the DL part:\n",
        "1. Similar to the tabular case, but now we'll create a 3D cube with the Sentinel-2 data instead of a point.\n",
        "2. Sample a specified number of farms (`sample_size = 4000`).\n",
        "\n",
        "And then send the request to The Cloud!!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f926ec3f",
      "metadata": {
        "id": "f926ec3f"
      },
      "source": [
        "### 2.1 Problem Scoping <font color='blue'>`Beginner`</font>\n",
        "\n",
        "Let's answer a few fundamental questions about the problem before we begin:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696a6f9f",
      "metadata": {
        "id": "696a6f9f"
      },
      "source": [
        "#### **What type of machine learning problem is this?**\n",
        "\n",
        "This is a supervised multiclass classification problem.\n",
        "\n",
        "#### **What inputs are we going to use?**\n",
        "\n",
        "We will use pixel-level Sentinel-2 satellite imagery as input to our model:\n",
        "\n",
        "<div style=\"text-align:center;\">\n",
        "    <figure>\n",
        "        <img width=\"750px\" src=\"https://images.ctfassets.net/qfhr9fiom9gi/l1YijPOaC0jOT4pIs4FFq/c308480fc0a7ecef82c6724a4113ed7c/pasted_image_0.png\" />\n",
        "        <figcaption style=\"font-size:small;\">Sentinel-2 Bands (reference <a href=\"https://www.mdpi.com/2072-4292/8/3/166\">paper</a>)</figcaption>\n",
        "    </figure>\n",
        "</div>\n",
        "\n",
        "- The input includes **12 bands of observations from Sentinel-2 L2A**: observations in the ultra-blue, blue, green, red; visible and near-infrared (VNIR); and short wave infrared (SWIR) spectra, as well as a cloud probability layer.\n",
        "- Each pixel has measurements for **13 dates** that cover the final part of winter, spring and early summer of 2018/2019.\n",
        "\n",
        "Details about the bands:\n",
        "- The twelve bands are\n",
        "    - B01 (Coastal aerosol)\n",
        "    - B02 (Blue)\n",
        "    - B03 (Green)\n",
        "    - B04 (Red)\n",
        "    - B05 (Red Edge 1)\n",
        "    - B06 (Red Edge 2)\n",
        "    - B07 (Red Edge 3)\n",
        "    - B08 (NIR - Near Infrared)\n",
        "    - B8A (Red Edge 4)\n",
        "    - B09 (Water vapor)\n",
        "    - B11 (SWIR - Shortwave Infrared 1)\n",
        "    - B12 (SWIR - Shortwave Infrared 2)\n",
        "- SCL (Scene Classification Map) was developed to distinguish between cloudy pixels, clear pixels and water pixels of Sentinel-2 data and is a result of the Scene classification algorithm run by ESA. Twelve different classifications are provided including classes of clouds, vegetation, soils/desert, water and snow. It does not constitute a land cover classification map in a strict sense.\n",
        "- The cloud probability layer is a product of the Sentinel-2 atmospheric correction algorithm (Sen2Cor) and provides an estimated cloud probability (0-100%) for the entire scene\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9284662",
      "metadata": {
        "id": "b9284662"
      },
      "source": [
        "#### **Can you Frame your Approach?**\n",
        "\n",
        "<div style=\"text-align:center;\">\n",
        "    <figure>\n",
        "        <img style=\"750px;\" src=\"https://i.postimg.cc/DZLB2ytm/Indaba-bd.png\" />\n",
        "        <figcaption style=\"font-size:small;\">Approach Overview.</figcaption>\n",
        "    </figure>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c0df8f",
      "metadata": {
        "id": "88c0df8f"
      },
      "source": [
        "#### **What are we predicting?**\n",
        "\n",
        "We need to **classify each field** into one of the following categories:\n",
        "\n",
        "| Clase Nivel 2                     | DV  |\n",
        "|-----------------------------------|-----|\n",
        "| 1.1 Bosque                       | 3   |\n",
        "| 2.1 Humedal                      | 11  |\n",
        "| 2.2 Pastizal                     | 12  |\n",
        "| 2.3 Matorral                     | 66  |\n",
        "| 2.4 Afloramiento rocoso          | 29  |\n",
        "| 3.1 Plantaci√≥n Forestal          | 9   |\n",
        "| 3.2 Mosaico de agricultura y pastura | 21  |\n",
        "| 4.1 Infraestructura              | 24  |\n",
        "| 4.2 Arenas, Playas y Dunas       | 23  |\n",
        "| 4.3 Salar                        | 61  |\n",
        "| 4.4 Otra √°rea sin vegetaci√≥n     | 25  |\n",
        "| 5.1 R√≠o, lago u oc√©ano           | 33  |\n",
        "| 5.2 Hielo y nieve                | 34  |\n",
        "| No observado                     | 27  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4de689f",
      "metadata": {
        "id": "e4de689f"
      },
      "source": [
        "#### **How will we validate the model?**\n",
        "\n",
        "We will conduct a random train-validation split by farm IDs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5db29db",
      "metadata": {
        "id": "b5db29db"
      },
      "source": [
        "#### **How will we measure performance?**\n",
        "\n",
        "The evaluation metric for training is **cross-entropy**. For each \"farm\" (predio_id), we are expected to predict the probability that the farm has a landcover of that type. Example:\n",
        "\n",
        "#### Prediction\n",
        "```\n",
        "predio_id     LandcoverId_1  LandcoverId_2  LandcoverId_3  LandcoverId_4  LandcoverId_5  LandcoverId_6  LandcoverId_7\n",
        "<integer>       <float>       <float>         <float>         <float>         <float>     <float>         <float>\n",
        "  1184            0.14         0.14            0.14            0.14           0.14        0.14            0.16\n",
        "```\n",
        "\n",
        "#### Target\n",
        "```\n",
        "predio_id     LandcoverId_1  LandcoverId_2  LandcoverId_3  LandcoverId_4  LandcoverId_5  LandcoverId_6  LandcoverId_7\n",
        "<integer>       <float>       <float>         <float>         <float>         <float>     <float>         <float>\n",
        "  1184            0              0              0                 0              0            0              1\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "The evaluation metric for validation is **F1-score**, plus a **confusion matrix** and some other metrics.\n",
        "\n",
        "Next, we want to prepare the dataset for machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "243fb088",
      "metadata": {
        "id": "243fb088"
      },
      "source": [
        "### 2.2 Model 1: Tabular ML with `LightGBM` <font color='orange'>`Intermediate`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1153322",
      "metadata": {
        "id": "a1153322"
      },
      "source": [
        "#### Data Preparation\n",
        "\n",
        "In this section, we want to do the following:\n",
        "\n",
        "- Split the data into train/validation/test.\n",
        "- Verify that no data leakage is present in the train/validation/test data.\n",
        "- Check the distribution of each channel or band.\n",
        "- Plot the farms by their labels in a map.\n",
        "- Visualize a single farm's NDVI as it changes through time (13 dates)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a02274b",
      "metadata": {
        "id": "0a02274b"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(data_dir / \"df.parquet\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4175a016",
      "metadata": {
        "id": "4175a016"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618a5c5c",
      "metadata": {
        "id": "618a5c5c"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b549a01",
      "metadata": {
        "id": "1b549a01"
      },
      "outputs": [],
      "source": [
        "# @title A little summary of the data\n",
        "print(f\"Unique number of fields: {df['predio_id'].nunique()}\")\n",
        "print(f\"Unique dates: {df['time'].nunique()}\")\n",
        "print(f\"Unique landcover classes: {df['landcover'].nunique()}\")\n",
        "print(f\"Unique number of points: {len(df[['y', 'x']].drop_duplicates())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c87312d",
      "metadata": {
        "id": "5c87312d"
      },
      "outputs": [],
      "source": [
        "# @title Plot the points on the map\n",
        "report = df.copy()\n",
        "report = report.sample(frac=1)\n",
        "report = report[[\"predio_id\", \"y\", \"x\"]].drop_duplicates()\n",
        "report = gpd.GeoDataFrame(\n",
        "    report, geometry=[Point(xy) for xy in zip(report[\"x\"], report[\"y\"])]\n",
        ")\n",
        "report = report[[\"predio_id\", \"geometry\"]].drop_duplicates(subset=\"predio_id\")\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "report.plot(ax=ax, color=\"red\", markersize=1)\n",
        "ax.set_title(\"Sample points\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2406ffe",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "d2406ffe"
      },
      "outputs": [],
      "source": [
        "# @title A look to the NDVI: time series of a single farm\n",
        "sample_id = df.predio_id.sample(1, random_state=563).values[0]\n",
        "dfs = df[df[\"predio_id\"] == sample_id].copy()\n",
        "dfs[\"NDVI\"] = (dfs[\"nir\"] - dfs[\"red\"]) / (dfs[\"nir\"] + dfs[\"red\"])\n",
        "\n",
        "agg_df = (\n",
        "    dfs.groupby([\"time\"])\n",
        "    .agg(\n",
        "        avg_value=(\"NDVI\", \"mean\"),\n",
        "        median_value=(\"NDVI\", \"median\"),\n",
        "        max_value=(\"NDVI\", \"max\"),\n",
        "        min_value=(\"NDVI\", \"min\"),\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "\n",
        "agg_df.plot(\n",
        "    x=\"time\", y=agg_df.columns.difference([\"time\"]), figsize=(10, 6), title=sample_id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b017252",
      "metadata": {
        "id": "9b017252"
      },
      "source": [
        "#### What is the NDVI?\n",
        "\n",
        "The Normalized Difference Vegetation Index (NDVI) is a simple, yet effective, vegetation index that is widely used in remote sensing applications. It is calculated as:\n",
        "\n",
        "$$\n",
        "\\text{NDVI} = \\frac{{NIR - Red}}{{NIR + Red}}\n",
        "$$\n",
        "\n",
        "Wwe wlll compute this and other indices in the following sections. They work more or less the same way, but they are made to capture different aspects of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc6c9d3",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "3dc6c9d3"
      },
      "outputs": [],
      "source": [
        "# @title A look to the NDVI: time series of an image\n",
        "d = dfs.set_index([\"time\", \"y\", \"x\"]).to_xarray()\n",
        "\n",
        "d[\"NDVI\"].plot.imshow(col=\"time\", x=\"x\", y=\"y\", col_wrap=5, figsize=(11, 7))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7502eef8",
      "metadata": {
        "id": "7502eef8",
        "lines_to_next_cell": 2
      },
      "source": [
        "#### <font color=\"red\"> **Exercise 2.5.** Check missing values\n",
        "Check if the the dataframe contains missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f6d98d",
      "metadata": {
        "id": "46f6d98d"
      },
      "source": [
        "There are many more things that we can explore with data. For now, let's skip ahead to the **modeling** section.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aa70800",
      "metadata": {
        "id": "1aa70800"
      },
      "source": [
        "#### Modeling\n",
        "\n",
        "In this section, we aim to train a `LightGBM` model to predict each farm's crop type by summarizing the historical band information. We will go over the following:\n",
        "\n",
        "<div style=\"text-align:center;\">\n",
        "    <figure>\n",
        "        <img style=\"width:66%;\" src=\"https://i.postimg.cc/8z3qzdJ0/Screenshot-2023-08-12-at-20-15-41.png\" />\n",
        "        <figcaption style=\"font-size:small;\">Note: period <b>1</b> and <b>5</b> are not shown as <b>they represent single dates</b> (not intervals).</figcaption>\n",
        "    </figure>\n",
        "</div>\n",
        "\n",
        "- Establishing the validation metric of a **frequency based model** that always predicts crop type frequencies derived from `y_train`.\n",
        "- Feature engineering: we will calculate the following S2-based indidces:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{NDVI} & = \\frac{{B08 - B04}}{{B08 + B04}} \\\\\n",
        "\\text{RDNDVI1} & = \\frac{{B08 - B05}}{{B08 + B05}} \\\\\n",
        "\\text{RDNDVI2} & = \\frac{{B08 - B06}}{{B08 + B06}} \\\\\n",
        "\\text{GCVI} & = \\frac{{B08}}{{B03}} - 1 \\\\\n",
        "\\text{RDGCVI1} & = \\frac{{B08}}{{B05}} - 1 \\\\\n",
        "\\text{RDGCVI2} & = \\frac{{B08}}{{B06}} - 1 \\\\\n",
        "\\text{MTCI} & = \\frac{{B08 - B05}}{{B05 - B04}} \\\\\n",
        "\\text{MTCI2} & = \\frac{{B06 - B05}}{{B05 - B04}} \\\\\n",
        "\\text{REIP} & = 700 + 40 \\left( \\frac{{(B04 + B07)/2 - B05}}{{B07 - B05}} \\right) \\\\\n",
        "\\text{NBR1} & = \\frac{{B08 - B11}}{{B08 + B11}} \\\\\n",
        "\\text{NBR2} & = \\frac{{B08 - B12}}{{B08 + B12}} \\\\\n",
        "\\text{NDTI} & = \\frac{{B11 - B12}}{{B11 + B12}} \\\\\n",
        "\\text{CRC} & = \\frac{{B11 - B03}}{{B11 + B03}} \\\\\n",
        "\\text{STI} & = \\frac{{B11}}{{B12}}\n",
        "\\end{align*}\n",
        "$$\n",
        "- **Spatial median-aggregation** by field `ID` and `time`.\n",
        "- Conduct **period-based temporal aggregation** and for each band and index, create period-based columns using the following temporal groups:\n",
        "    - `period 1`\n",
        "        - *2018-09-01*\n",
        "        - *2018-09-21*\n",
        "    - `period 2`\n",
        "        - *2018-10-21*\n",
        "        - *2018-10-26*\n",
        "    - `period 3`\n",
        "        - *2018-11-20*\n",
        "        - *2018-12-10*\n",
        "        - *2018-12-15*\n",
        "    - `period 4`\n",
        "        - *2018-12-20*\n",
        "        - *2018-12-25*\n",
        "        - *2018-12-30*\n",
        "    - `period 5`\n",
        "        - *2019-01-04*\n",
        "        - *2019-01-24*\n",
        "        - *2019-01-29*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f646a088",
      "metadata": {
        "id": "f646a088"
      },
      "source": [
        "#### Frequency-Based Baseline Model\n",
        "\n",
        "Establishing a baseline is crucial in machine learning to set a reference point for model performance. By using a basic approach, we can better gauge the effectiveness of more sophisticated models developed later.\n",
        "\n",
        "The idea behind this frequency-based baseline is straightforward:\n",
        "\n",
        "1. **Compute Class Frequencies**: Determine the proportion (or frequency) of each crop type present in the training data (`y` targets).\n",
        "2. **Use Frequencies for Prediction**: For each validation sample, predict the probability of each class based on these frequencies, implying that the validation set would exhibit a similar class distribution as the training set.\n",
        "\n",
        "This method provides a rudimentary performance measure. Any advanced models developed should aim to surpass this baseline metric, ensuring that our modeling efforts add genuine value."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6a0d104",
      "metadata": {
        "id": "f6a0d104",
        "lines_to_next_cell": 2
      },
      "source": [
        "#### `LightGBM`\n",
        "\n",
        "We will create functions that cover the data preparation steps in the original section description.\n",
        "\n",
        "Let's implement the feature engineering function that would add additional vegetation indices of interest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "906edb15",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "906edb15"
      },
      "outputs": [],
      "source": [
        "# @title Function: Calculate spectral indices\n",
        "def calculate_indices(df):\n",
        "    \"\"\"\n",
        "    Calculate spectral indices from Sentinel-2 bands.\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Input DataFrame with band columns.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        DataFrame with added spectral indices.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df = df.copy()\n",
        "\n",
        "    # Calculate indices\n",
        "    # Normalized Difference Vegetation Index (NDVI)\n",
        "    df[\"NDVI\"] = (df[\"nir\"] - df[\"red\"]) / (df[\"nir\"] + df[\"red\"])\n",
        "\n",
        "    # Red-edge Normalized Difference Vegetation Index (RDNDVI)\n",
        "    df[\"RDNDVI1\"] = (df[\"nir\"] - df[\"rededge1\"]) / (df[\"nir\"] + df[\"rededge1\"])\n",
        "    df[\"RDNDVI2\"] = (df[\"nir\"] - df[\"rededge2\"]) / (df[\"nir\"] + df[\"rededge2\"])\n",
        "\n",
        "    # Green Chlorophyll Vegetation Index (GCVI)\n",
        "    df[\"GCVI\"] = df[\"nir\"] / df[\"green\"] - 1\n",
        "\n",
        "    # Red-edge GCVI\n",
        "    df[\"RDGCVI1\"] = df[\"nir\"] / df[\"rededge1\"] - 1\n",
        "    df[\"RDGCVI2\"] = df[\"nir\"] / df[\"rededge2\"] - 1\n",
        "\n",
        "    # Meris Terrestrial Chlorophyll Index (MTCI)\n",
        "    # df['MTCI'] = (df['B08'] - df['B05']) / (df['B05'] - df['B04'])\n",
        "    # df['MTCI2'] = (df['B06'] - df['B05']) / (df['B05'] - df['B04'])\n",
        "\n",
        "    # Red-edge Inflection Point (REIP)\n",
        "    # df['REIP'] = 700 + 40 * (((df['B04'] + df['B07']) / 2) - df['B05']) / (df['B07'] - df['B05'])\n",
        "\n",
        "    # Normalized Burn Ratio (NBR)\n",
        "    df[\"NBR1\"] = (df[\"nir\"] - df[\"swir16\"]) / (df[\"nir\"] + df[\"swir16\"])\n",
        "    df[\"NBR2\"] = (df[\"nir\"] - df[\"swir22\"]) / (df[\"nir\"] + df[\"swir22\"])\n",
        "\n",
        "    # Normalized Difference Tillage Index (NDTI)\n",
        "    df[\"NDTI\"] = (df[\"swir16\"] - df[\"swir22\"]) / (df[\"swir16\"] + df[\"swir22\"])\n",
        "\n",
        "    # Canopy Chlorophyll Content Index (CRC)\n",
        "    df[\"CRC\"] = (df[\"swir16\"] - df[\"green\"]) / (df[\"swir16\"] + df[\"green\"])\n",
        "\n",
        "    # Soil Tillage Index (STI)\n",
        "    df[\"STI\"] = df[\"swir16\"] / df[\"swir22\"]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc718b7",
      "metadata": {
        "id": "cdc718b7",
        "lines_to_next_cell": 2
      },
      "source": [
        "We also need functions for spatial and temporal aggregation to reduce the dimensionality of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2485316",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "a2485316"
      },
      "outputs": [],
      "source": [
        "# @title Functions: Spatial median and period based aggregation\n",
        "def spatial_median_aggregation(df, bands):\n",
        "    \"\"\"\n",
        "    Aggregate data by field and time, using the median of band values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Input DataFrame with 'field', 'time', and band columns.\n",
        "    bands : list\n",
        "        List of band columns to be aggregated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Aggregated DataFrame with median band values.\n",
        "    \"\"\"\n",
        "    # Calculate median of band values for each unique 'field' and 'time'\n",
        "    agg_df = df.groupby([\"predio_id\", \"time\"])[bands].median().reset_index()\n",
        "\n",
        "    # Drop duplicate entries for each unique 'field' and 'time', and remove band columns\n",
        "    unique_df = df.drop_duplicates([\"predio_id\", \"time\"]).drop(bands, axis=1)\n",
        "\n",
        "    # Merge aggregated DataFrame with unique DataFrame\n",
        "    return pd.merge(agg_df, unique_df, on=[\"predio_id\", \"time\"])\n",
        "\n",
        "\n",
        "def period_based_aggregation(df, bands):\n",
        "    \"\"\"\n",
        "    Aggregate data by field and defined time periods, using the mean of band values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Input DataFrame with 'field', 'time', and band columns.\n",
        "    bands : list\n",
        "        List of band columns to be aggregated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Aggregated DataFrame with mean band values for each time period.\n",
        "    \"\"\"\n",
        "    # Define time periods\n",
        "    periods = {\n",
        "        \"p1\": pd.to_datetime([\"2018-09-01\", \"2018-09-21\"]).date,  # September\n",
        "        \"p2\": pd.to_datetime([\"2018-10-21\", \"2018-10-26\"]).date,  # October\n",
        "        \"p3\": pd.to_datetime(\n",
        "            [\"2018-11-20\", \"2018-12-10\", \"2018-12-15\"]\n",
        "        ).date,  # November-Early December\n",
        "        \"p4\": pd.to_datetime(\n",
        "            [\"2018-12-20\", \"2018-12-25\", \"2018-12-30\"]\n",
        "        ).date,  # Late December\n",
        "        \"p5\": pd.to_datetime(\n",
        "            [\"2019-01-04\", \"2019-01-24\", \"2019-01-29\"]\n",
        "        ).date,  # January\n",
        "    }\n",
        "\n",
        "    # Convert time column to datetime if not already\n",
        "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "\n",
        "    # Assign period labels based on 'time'\n",
        "    df[\"period\"] = None\n",
        "    for period, dates in periods.items():\n",
        "        df.loc[df[\"time\"].dt.date.isin(dates), \"period\"] = period\n",
        "\n",
        "    # Calculate mean of band values for each unique 'field' and 'period'\n",
        "    period_agg_df = df.groupby([\"predio_id\", \"period\"])[bands].mean().reset_index()\n",
        "\n",
        "    # Drop duplicate entries for each unique 'field' and 'period', and remove band columns\n",
        "    unique_df = df.drop_duplicates([\"predio_id\", \"period\"]).drop(bands, axis=1)\n",
        "\n",
        "    # Merge aggregated DataFrame with unique DataFrame\n",
        "    return pd.merge(period_agg_df, unique_df, on=[\"predio_id\", \"period\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5aca024",
      "metadata": {
        "id": "d5aca024",
        "lines_to_next_cell": 2
      },
      "source": [
        "Finally, we create functions to pivot the table (making periods into columns) and another function that runs the steps and splits the dataframe into `X` and `y`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "299dd869",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "299dd869"
      },
      "outputs": [],
      "source": [
        "# @title Function: Pivot the dataframe\n",
        "def pivot_dataframe(df, odf):\n",
        "    \"\"\"\n",
        "    Pivot the DataFrame so that each time period becomes a separate column.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Input DataFrame with 'field', 'period', and other columns.\n",
        "    odf : pandas.DataFrame\n",
        "        Original DataFrame with 'x', 'y', and other columns.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Pivoted DataFrame with each 'period' as a separate column.\n",
        "    \"\"\"\n",
        "    godf = (\n",
        "        odf.groupby([\"predio_id\"]).agg({\"x\": \"mean\", \"y\": \"mean\"}).round().reset_index()\n",
        "    )\n",
        "    df_ = df.drop(columns=[\"x\", \"y\"]).merge(godf)\n",
        "\n",
        "    return (\n",
        "        df_.pivot(index=[\"predio_id\", \"landcover\", \"y\", \"x\"], columns=\"period\")\n",
        "        .fillna(-1)\n",
        "        .reset_index()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3efafb75",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "3efafb75"
      },
      "source": [
        "Let's prepare the training, validation, and test arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d938480",
      "metadata": {
        "id": "1d938480"
      },
      "outputs": [],
      "source": [
        "# @title Start data processing\n",
        "#\n",
        "# Select relevant features and preprocess data\n",
        "bands = [\n",
        "    \"coastal\",\n",
        "    \"blue\",\n",
        "    \"green\",\n",
        "    \"red\",\n",
        "    \"rededge1\",\n",
        "    \"rededge2\",\n",
        "    \"rededge3\",\n",
        "    \"nir\",\n",
        "    \"nir09\",\n",
        "    \"swir16\",\n",
        "    \"swir22\",\n",
        "    \"nir08\",\n",
        "]\n",
        "indices = [\n",
        "    \"NDVI\",\n",
        "    \"RDNDVI1\",\n",
        "    \"RDNDVI2\",\n",
        "    \"GCVI\",\n",
        "    \"RDGCVI1\",\n",
        "    \"RDGCVI2\",\n",
        "    \"NBR1\",\n",
        "    \"NBR2\",\n",
        "    \"NDTI\",\n",
        "    \"CRC\",\n",
        "    \"STI\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f85b76",
      "metadata": {
        "id": "18f85b76"
      },
      "outputs": [],
      "source": [
        "# @title Filter data by cloud cover and scene classification\n",
        "df_filtered = df[(df.scene_cloud_cover < 0.6) & (df.scl.isin([4, 5, 6, 7]))]\n",
        "print(\"\\nClass distribution after filtering:\")\n",
        "print(df_filtered.landcover.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73dd347c",
      "metadata": {
        "id": "73dd347c"
      },
      "outputs": [],
      "source": [
        "# @title Calculate all and split train/test/val\n",
        "# Calculate spectral indices\n",
        "df_filtered = calculate_indices(df_filtered)\n",
        "\n",
        "# Aggregate data spatially using median\n",
        "df_spatial = spatial_median_aggregation(df_filtered, bands + indices)\n",
        "print(\"\\nClass distribution after spatial aggregation:\")\n",
        "print(df_spatial.landcover.value_counts())\n",
        "\n",
        "# Aggregate data by time periods using mean\n",
        "df_temporal = period_based_aggregation(df_spatial, bands + indices)\n",
        "print(\"\\nClass distribution after temporal aggregation:\")\n",
        "print(df_temporal.landcover.value_counts())\n",
        "\n",
        "# Pivot the data to have each period as a separate column\n",
        "df_pivoted = pivot_dataframe(df_temporal, df)\n",
        "print(\"\\nClass distribution after pivot:\")\n",
        "print(df_pivoted.landcover.value_counts())\n",
        "\n",
        "# Flatten multi-level column names\n",
        "df_pivoted.columns = [\n",
        "    \"\".join(col).strip() if isinstance(col, tuple) else col\n",
        "    for col in df_pivoted.columns.values\n",
        "]\n",
        "\n",
        "# Select columns to keep\n",
        "columns_to_keep = [\"predio_id\", \"y\", \"x\", \"landcover\"] + [\n",
        "    col\n",
        "    for col in df_pivoted.columns\n",
        "    if col.endswith((\"p1\", \"p2\", \"p3\", \"p4\", \"p5\"))\n",
        "    and not col.startswith((\"time\", \"y\", \"x\", \"landcover\"))\n",
        "]\n",
        "df_final = df_pivoted[columns_to_keep]\n",
        "\n",
        "# Create label mapping\n",
        "original_labels = df_final.landcover.unique()\n",
        "label_mapping = {label: idx for idx, label in enumerate(np.unique(original_labels))}\n",
        "print(\"\\nLabel mapping:\", label_mapping)\n",
        "\n",
        "# Calculate class weights\n",
        "unique_classes = np.unique(df_final.landcover.replace(label_mapping))\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=unique_classes,\n",
        "    y=df_final.landcover.replace(label_mapping),\n",
        ")\n",
        "print(\"\\nClass weights:\", {i: w for i, w in enumerate(class_weights)})\n",
        "\n",
        "# Split features and target\n",
        "X = df_final.drop([\"landcover\"], axis=1)\n",
        "y = df_final[\"landcover\"].replace(\n",
        "    label_mapping\n",
        ")  # we need to replace for continutos labels!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ac6546",
      "metadata": {
        "id": "e4ac6546"
      },
      "outputs": [],
      "source": [
        "# Print class distribution in train/val/test sets\n",
        "print(\"\\nSplitting data...\")\n",
        "unique_ids = df_final[\"predio_id\"].unique()\n",
        "train_ids, temp_ids = train_test_split(\n",
        "    unique_ids,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=df_final.groupby(\"predio_id\")[\"landcover\"].first(),  # Stratify by label\n",
        ")\n",
        "\n",
        "val_ids, test_ids = train_test_split(\n",
        "    temp_ids,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=df_final[df_final[\"predio_id\"].isin(temp_ids)]\n",
        "    .groupby(\"predio_id\")[\"landcover\"]\n",
        "    .first(),\n",
        ")\n",
        "\n",
        "# Create splits based on the IDs\n",
        "X_train = X[X[\"predio_id\"].isin(train_ids)]\n",
        "X_val = X[X[\"predio_id\"].isin(val_ids)]\n",
        "X_test = X[X[\"predio_id\"].isin(test_ids)]\n",
        "\n",
        "y_train = y[X[\"predio_id\"].isin(train_ids)]\n",
        "y_val = y[X[\"predio_id\"].isin(val_ids)]\n",
        "y_test = y[X[\"predio_id\"].isin(test_ids)]\n",
        "\n",
        "print(\"\\nClass distribution in train set:\")\n",
        "print(np.bincount(y_train))\n",
        "print(\"\\nClass distribution in validation set:\")\n",
        "print(np.bincount(y_val))\n",
        "print(\"\\nClass distribution in test set:\")\n",
        "print(np.bincount(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e1f72f",
      "metadata": {
        "id": "a3e1f72f"
      },
      "outputs": [],
      "source": [
        "# @title  Check data leakage\n",
        "col = \"predio_id\"\n",
        "assert len(set(X_train[col].unique()).intersection(set(X_val[col].unique()))) == 0\n",
        "assert len(set(X_train[col].unique()).intersection(set(X_test[col].unique()))) == 0\n",
        "assert len(set(X_val[col].unique()).intersection(set(X_test[col].unique()))) == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97a9283f",
      "metadata": {
        "id": "97a9283f"
      },
      "outputs": [],
      "source": [
        "# @title Let's create utility dictionaries (for the future)\n",
        "dv_to_label = {\n",
        "    3: \"Bosque\",\n",
        "    11: \"Humedal\",\n",
        "    12: \"Pastizal\",\n",
        "    66: \"Matorral\",\n",
        "    29: \"Afloramiento rocoso\",\n",
        "    9: \"Plantaci√≥n Forestal\",\n",
        "    21: \"Agricultura y pastura\",\n",
        "    24: \"Infraestructura\",\n",
        "    23: \"Arenas, Playas y Dunas\",\n",
        "    61: \"Salar\",\n",
        "    25: \"Otra √°rea sin vegetaci√≥n\",\n",
        "    33: \"R√≠o, lago u oc√©ano\",\n",
        "    34: \"Hielo y nieve\",\n",
        "    27: \"No observado\",\n",
        "}\n",
        "\n",
        "class_to_label = {\n",
        "    label_mapping[key]: dv_to_label[key] for key in dv_to_label if key in label_mapping\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d9cfa6d",
      "metadata": {
        "id": "2d9cfa6d"
      },
      "outputs": [],
      "source": [
        "# @title What's going \"in\" the model?\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ae0213",
      "metadata": {
        "id": "80ae0213"
      },
      "outputs": [],
      "source": [
        "X_train.columns.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "564aa57b",
      "metadata": {
        "id": "564aa57b",
        "lines_to_next_cell": 2
      },
      "source": [
        "##### <font color=\"red\"> **Exercise 2.6.** Exercise 2.6 Calculate the cross entropy loss? </font>\n",
        "(*Hint: import the cross entropy loss from sklearn*)\n",
        "\n",
        "Cross-entropy loss measures the difference between two probability distributions - the true distribution \\( $P$ \\) and the predicted distribution \\( $Q$ \\). For classification tasks, it's defined as:\n",
        "\n",
        "$H(P, Q) = -\\sum_{i} P(i) \\log Q(i)$\n",
        "\n",
        "Where \\( $P_i$ \\) is the true probability (1 for the correct class, 0 otherwise) and \\( Q_i \\) is the predicted probability. In machine learning, we minimize this loss to improve model accuracy. It's particularly useful for:\n",
        "1. Multi-class classification problems\n",
        "2. Probabilistic outputs\n",
        "3. Hyperparameter optimization\n",
        "\n",
        "During hyperparameter search (training), we use cross-entropy as the objective function to evaluate different model configurations. Lower cross-entropy indicates better alignment between predictions and true labels, guiding the search towards optimal hyperparameters.\n",
        "\n",
        "In a regression, we would use the mean squared error loss (MSE), but as this is a classification problem, we use the cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97527334",
      "metadata": {
        "cellView": "form",
        "id": "97527334"
      },
      "outputs": [],
      "source": [
        "# @title Calculate the class frequencies from `y_train` in order to generate the baseline predictions\n",
        "y_val_hat = np.repeat(\n",
        "    y_train.value_counts(normalize=True).sort_index().values[None, ...],\n",
        "    y_val.shape[0],\n",
        "    axis=0,\n",
        ")\n",
        "y_val_hat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6894d951",
      "metadata": {
        "id": "6894d951"
      },
      "outputs": [],
      "source": [
        "# @title Answer to Exercise 2.6 (Try not to peek until you've given it a good try!')\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Calculate cross-entropy\n",
        "cross_entropy = log_loss(y_val, y_val_hat)\n",
        "\n",
        "print(f\"Cross-entropy is {cross_entropy}\")\n",
        "\n",
        "# .. to be used for comparison\n",
        "baseline_ce = cross_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0092db0d",
      "metadata": {
        "id": "0092db0d"
      },
      "source": [
        "Any model that we construct should have a validation cross-entropy less than the baseline cross-entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437c8497",
      "metadata": {
        "id": "437c8497"
      },
      "source": [
        "Now, let's conduct random hyperparameter search with cross-validation using the `LightGBM` estimator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acaf613f",
      "metadata": {
        "id": "acaf613f"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a145ab7c",
      "metadata": {
        "id": "a145ab7c"
      },
      "outputs": [],
      "source": [
        "# --- it takes ~ 4 to 10 min\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the LightGBM model\n",
        "model = lgb.LGBMClassifier(\n",
        "    objective=\"multiclass\", verbose=-1, num_class=len(original_labels)\n",
        ")\n",
        "banned_cols = [\"predio_id\"]\n",
        "\n",
        "# Define the hyperparameters space\n",
        "param_dist = {\n",
        "    \"num_leaves\": [31, 127, 200, 300],\n",
        "    \"reg_alpha\": [0.1, 0.5],\n",
        "    \"min_data_in_leaf\": [30, 50, 100, 300, 400],\n",
        "    \"lambda_l1\": [0, 1, 1.5],\n",
        "    \"lambda_l2\": [0, 1],\n",
        "}\n",
        "\n",
        "# Define the scorer (loss function)\n",
        "scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
        "\n",
        "# Randomized Search for hyperparameter tuning\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,\n",
        "    scoring=scorer,\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "random_search.fit(X_train.drop(banned_cols, axis=1), y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21556c6c",
      "metadata": {
        "id": "21556c6c"
      },
      "source": [
        "#### Evaluation: Cross-entropy\n",
        "\n",
        "We re-train the best estimator on the training data and get the validation cross-entropy first, and then the performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a850d15",
      "metadata": {
        "id": "6a850d15"
      },
      "outputs": [],
      "source": [
        "# Create the LightGBM model instance with the best hyperparameters\n",
        "model = lgb.LGBMClassifier(\n",
        "    objective=\"multiclass\",\n",
        "    num_class=len(np.unique(y)),\n",
        "    verbose=-1,\n",
        "    **random_search.best_params_,\n",
        ")\n",
        "\n",
        "# Fit the model to the training set\n",
        "model.fit(X_train.drop(banned_cols, axis=1), y_train)\n",
        "\n",
        "# Predict the validation set results\n",
        "y_val_hat = model.predict_proba(X_val.drop(banned_cols, axis=1))\n",
        "\n",
        "model_ce = log_loss(y_val, y_val_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91f606e3",
      "metadata": {
        "id": "91f606e3"
      },
      "outputs": [],
      "source": [
        "# Report cross-entropy\n",
        "print(f\"Cross-entropy with best hyperparameters is {log_loss(y_val, y_val_hat):.5f}\")\n",
        "print(f\"Base Cross-entropy is: {baseline_ce}\")\n",
        "print(\n",
        "    f\"It is {100 * (baseline_ce - model_ce) / baseline_ce:.2f}% better (smaller) than the baseline\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "357360bb",
      "metadata": {
        "id": "357360bb"
      },
      "source": [
        "#### Evaluation: Perfomance metrics\n",
        "\n",
        "We're going to investigate Class-Imbalances with the performance metrics.\n",
        "\n",
        "Let's report the following metrics on the combination of validation + test points:\n",
        "- `Precision`\n",
        "- `Recall`\n",
        "- `F1`\n",
        "- `Confusion matrix`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a769ccf",
      "metadata": {
        "id": "0a769ccf"
      },
      "outputs": [],
      "source": [
        "# Predict the validation set results\n",
        "y_test_hat = model.predict(pd.concat([X_val, X_test]).drop(banned_cols, axis=1))\n",
        "y_test_arr = pd.concat([y_val, y_test]).values\n",
        "y_test_hat.shape, y_test_arr.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d01c41",
      "metadata": {
        "id": "94d01c41"
      },
      "source": [
        "#### <font color=\"red\"> Exercise 2.7 Calculate precision, recall, and F1 score? </font>\n",
        "(*Hint: for average variable use \"average = 'weighted*, for more information you can check example: precision_score?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef7e0b9",
      "metadata": {
        "id": "0ef7e0b9"
      },
      "outputs": [],
      "source": [
        "# @title Exercise 2.7 Calculate precision, recall, and F1 score?\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = None  # update me\n",
        "recall = None  # update me\n",
        "f1 = None  # update me\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf75fbf3",
      "metadata": {
        "id": "cf75fbf3",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# @title Answer to Exercise 2.7\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test_arr, y_test_hat, average=\"weighted\")\n",
        "recall = recall_score(y_test_arr, y_test_hat, average=\"weighted\")\n",
        "f1 = f1_score(y_test_arr, y_test_hat, average=\"weighted\")\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Why weighted average? What does it mean? What are the options?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2e1601",
      "metadata": {
        "id": "bc2e1601"
      },
      "outputs": [],
      "source": [
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test_arr, y_test_hat, normalize=\"true\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(\n",
        "    cm, annot=True, xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys()\n",
        ")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a38e3f5",
      "metadata": {
        "id": "3a38e3f5"
      },
      "source": [
        "All classes are relatively well classified, except `24` and `25`, where around 30% of the farms were missclassified. Since cross-entropy function loss does not mitigate against class imbalance, we still get a good score.\n",
        "\n",
        "*Hint: try changing `LGBMClassifier`'s `class_weight` attribute to `balanced`.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa20f3cf",
      "metadata": {
        "id": "aa20f3cf"
      },
      "source": [
        "#### Explainable Artificial Intelligence (XAI)\n",
        "\n",
        "What are the most important periods and indices?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb272e8f",
      "metadata": {
        "id": "cb272e8f"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# Prepare the validation + test data for the model\n",
        "X_vt = pd.concat([X_val, X_test])\n",
        "\n",
        "# explain the model's predictions using SHAP\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_vt.drop(banned_cols, axis=1))\n",
        "\n",
        "shap.summary_plot(shap_values, X_vt.drop(banned_cols, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43e6f1f1",
      "metadata": {
        "id": "43e6f1f1"
      },
      "source": [
        "Let's figure out which periods are the most important:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e4b51d0",
      "metadata": {
        "id": "3e4b51d0"
      },
      "outputs": [],
      "source": [
        "shap_values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "493f7e3c",
      "metadata": {
        "id": "493f7e3c"
      },
      "outputs": [],
      "source": [
        "# Compute the absolute SHAP values for each feature\n",
        "abs_shap_values = np.sum(np.abs(shap_values), axis=(0, 2))\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = X_vt.drop(banned_cols, axis=1).columns\n",
        "\n",
        "# Create a DataFrame linking feature names to their importance\n",
        "feature_importances = pd.DataFrame(\n",
        "    {\"feature\": feature_names, \"importance\": abs_shap_values}\n",
        ")\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "feature_importances = feature_importances.sort_values(\"importance\", ascending=False)\n",
        "\n",
        "# Drop `lat` and `lon` from the dataset (WARNING: can we skip this?)\n",
        "feature_importances = feature_importances[\n",
        "    ~feature_importances[\"feature\"].isin([\"y\", \"x\"])\n",
        "]\n",
        "\n",
        "# Normalize the feature importances to sum to one\n",
        "feature_importances[\"importance\"] = (\n",
        "    feature_importances[\"importance\"] / feature_importances[\"importance\"].sum()\n",
        ")\n",
        "\n",
        "# Split the feature name into `index` and `period` (period is the last two characters)\n",
        "feature_importances[\"period\"] = feature_importances[\"feature\"].str[-2:]\n",
        "feature_importances[\"index\"] = feature_importances[\"feature\"].str[:-2]\n",
        "feature_importances = feature_importances.drop(\"feature\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989a4220",
      "metadata": {
        "id": "989a4220"
      },
      "outputs": [],
      "source": [
        "# Get the most important periods separately by aggregating their importance\n",
        "periods = (\n",
        "    feature_importances.drop(\"index\", axis=1)\n",
        "    .groupby(\"period\")\n",
        "    .sum()\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        ")\n",
        "\n",
        "# Get the most important bands separately by aggregating their importance\n",
        "bands = (\n",
        "    feature_importances.drop(\"period\", axis=1)\n",
        "    .groupby(\"index\")\n",
        "    .sum()\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f7a121",
      "metadata": {
        "id": "c4f7a121"
      },
      "outputs": [],
      "source": [
        "periods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557fed81",
      "metadata": {
        "id": "557fed81"
      },
      "outputs": [],
      "source": [
        "bands.iloc[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3bd2ff4",
      "metadata": {
        "id": "b3bd2ff4"
      },
      "source": [
        "We highlight the following top indices based on feature importance:\n",
        "\n",
        "- `CRC`: Canopy Chlorophyll Content Index. It is used to estimate the chlorophyll content in plant canopies.\n",
        "- `nir09`: Band 9 of Sentinel-2. It is a water vapor band, useful for atmospheric studies.\n",
        "- `coastal`: Band 1 of Sentinel-2. This is a coastal aerosol band and captures light in the blue portion of the electromagnetic spectrum.\n",
        "- `GCVI`: Green Chlorophyll Vegetation Index. It is used to estimate chlorophyll content and vegetation health.\n",
        "- `RDNDVI2`: Ratio Divergence Normalized Difference Vegetation Index 2. This is a custom vegetation index that uses a ratio and divergence calculation to normalize the vegetation index.\n",
        "- `green`: Band 3 of Sentinel-2. This is a green band used for vegetation analysis and chlorophyll content estimation.\n",
        "- `swir16`: Band 11 of Sentinel-2. This is a Shortwave Infrared 1 band used for vegetation and soil moisture analysis.\n",
        "- `blue`: Band 2 of Sentinel-2. This is a blue band used for water body and vegetation analysis.\n",
        "- `NDTI`: Normalized Difference Tillage Index. It is used to assess soil tillage conditions and residue cover.\n",
        "- `NDVI`: Normalized Difference Vegetation Index. It is used to assess vegetation health and density."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d6b890c",
      "metadata": {
        "id": "9d6b890c"
      },
      "source": [
        "#### Inference\n",
        "\n",
        "In this section, we will report the final metrics on the validation set and visualize the farms with their crop types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd78992",
      "metadata": {
        "id": "5cd78992"
      },
      "outputs": [],
      "source": [
        "# Predict on the test set\n",
        "y_test_pred = model.predict_proba(X_test.drop(banned_cols, axis=1))\n",
        "y_test_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0161fbd1",
      "metadata": {
        "id": "0161fbd1"
      },
      "outputs": [],
      "source": [
        "# Export the results\n",
        "report = X_test[[\"predio_id\"]].copy()\n",
        "cols = list(label_mapping.keys())\n",
        "report[cols] = y_test_pred.round(3)\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65b536d",
      "metadata": {
        "id": "d65b536d"
      },
      "outputs": [],
      "source": [
        "# @title Let's visualize the predicted test farms\n",
        "def create_geometry(df):\n",
        "    coords = list(zip(df.x, df.y))\n",
        "    if len(coords) == 1:\n",
        "        return Point(coords[0])\n",
        "    elif len(coords) == 2:\n",
        "        return LineString(coords)\n",
        "    else:\n",
        "        return Polygon(coords)\n",
        "\n",
        "\n",
        "# Create the polygons from the test set\n",
        "d = X_test.copy()\n",
        "cols = [\"predio_id\", \"y\", \"x\"]\n",
        "d = d[cols].drop_duplicates()\n",
        "d = (\n",
        "    d.groupby(\"predio_id\")\n",
        "    .apply(create_geometry, include_groups=False)\n",
        "    .reset_index()\n",
        "    .rename(columns={0: \"geometry\"})\n",
        ")\n",
        "\n",
        "# Create the dataframe to hold the pixel locations and the predicted landcover types\n",
        "report = X_test.copy()\n",
        "report = report[[\"predio_id\"]]\n",
        "report[\"landcover\"] = y_test_pred.argmax(axis=1) + 1\n",
        "\n",
        "# Merge the two dataframes\n",
        "report = report.merge(d, on=\"predio_id\", how=\"left\").rename(columns={0: \"geometry\"})\n",
        "report = gpd.GeoDataFrame(report, geometry=\"geometry\")\n",
        "\n",
        "# Replace the 'landcover' column with mapped names\n",
        "report[\"landcover\"] = report[\"landcover\"].map({v: k for k, v in label_mapping.items()})\n",
        "report[\"landcover\"] = report[\"landcover\"].astype(\"Int64\").astype(\"category\")\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "# Plot the GeoDataFrame using the 'landcover' column to color the polygons\n",
        "report.plot(\n",
        "    column=\"landcover\",\n",
        "    legend=True,\n",
        "    ax=ax,\n",
        "    cmap=\"tab20\",\n",
        "    categorical=True,\n",
        "    legend_kwds={\"bbox_to_anchor\": (1, 1)},\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37fa010b",
      "metadata": {
        "id": "37fa010b"
      },
      "source": [
        "---\n",
        "\n",
        "### 2.3 Model 2: Deep Learning <font color='purple'>`Experimental`</font>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/u/0/uc?id=1pd_-Azeunh7O7SoxXWhrZ5_f9O4wq1tU&export=download\" width=\"750px;\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe4c9145",
      "metadata": {
        "id": "fe4c9145"
      },
      "source": [
        "#### Data Preprocessing\n",
        "\n",
        "We have conducted the following steps to produce the `X.npy` and `y.npy` files:\n",
        "1. **Data normalization**: squared-root all bands in the Sentinel-2 imagery.\n",
        "2. **Farm masking**: Added an extra binary band that shows where the farm pixels are.\n",
        "3. **Farm Extraction**: For each field polygon, we get its center location and crop a 32x32 window from the original image to create the patches.\n",
        "4. **Patch Standardization**: Each patch was standardized using the temporal band-wise mean and standard deviation.\n",
        "5. **Creating a single Cube**: stacked all patches and their labels into 2 unified cubes and saved them as `X.npy` and `y.npy`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac4fe18",
      "metadata": {
        "id": "0ac4fe18"
      },
      "source": [
        "We need to load 2 files:\n",
        "- X.npy: a numpy array of shape (predio_id, band, time, y, x). The bands we have, are the ones that comes with Sentinel-2 data, plus the mask band.\n",
        "- y.npy: a numpy array of shape (predio_id,) with the lancover cv value.\n",
        "\n",
        "The dimensions have this data:\n",
        "1. predio_id: 3825 sampled fields (different from the previous dataset used for ML).\n",
        "2. band: 13 (Sentinel-2 bands + mask band): \"coastal\", \"blue\", \"green\", \"red\", \"rededge1\", \"rededge2\", \"rededge3\", \"nir\", \"nir09\", \"swir16\", \"swir22\", \"nir08\", \"mask\".\n",
        "3. time: 13 (13 times of the Sentinel-2 data).\n",
        "4. y: 32 (height of the patch).\n",
        "5. x: 32 (width of the patch).\n",
        "\n",
        "For now, this will be all the data that will be used as input to the model.\n",
        "\n",
        "Let's download the data for deep learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69dddceb",
      "metadata": {
        "id": "69dddceb"
      },
      "outputs": [],
      "source": [
        "# @title Next, we need to download the necessary files to be used in this practical:. (Run Cell)\n",
        "data_dir = Path(\"./data\")\n",
        "if data_dir.exists():\n",
        "    shutil.rmtree(data_dir, ignore_errors=True)\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "dl_arrays_url = \"https://drive.google.com/uc?id=16tUiT6rSJYGpXnNm6fIkPY0tJlNjmKKk\"\n",
        "\n",
        "# Download\n",
        "if not os.path.isfile(data_dir / \"deep_learning_arrays.zip\"):\n",
        "    gdown.download(\n",
        "        dl_arrays_url, str(data_dir / \"deep_learning_arrays.zip\"), quiet=False\n",
        "    )\n",
        "    with zipfile.ZipFile(\n",
        "        os.path.join(data_dir, \"deep_learning_arrays.zip\"), \"r\"\n",
        "    ) as zip_ref:\n",
        "        zip_ref.extractall(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6671d49c",
      "metadata": {
        "id": "6671d49c"
      },
      "outputs": [],
      "source": [
        "X = np.load(data_dir / \"X.npz\")[\"X\"]\n",
        "y_original = np.load(data_dir / \"y.npz\")[\"y\"]\n",
        "\n",
        "unique_y_values = sorted(list(set(y_original)))\n",
        "y_mapping = {val: i for i, val in enumerate(unique_y_values)}\n",
        "y = np.array([y_mapping[val] for val in y_original])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "401f0587",
      "metadata": {
        "id": "401f0587",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "X.shape  # (farm, band, time, y, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4da1cec",
      "metadata": {
        "id": "c4da1cec"
      },
      "source": [
        "#### Class Implementations\n",
        "\n",
        "As we now have the NumPy arrays for training, we want to implement a sequence-to-one classification model.\n",
        "\n",
        "Here are the choices that we are going to implement:\n",
        "- Encode each image using a pre-trained encoder (ResNet18).\n",
        "- Pass the sequence of encodings to a 3-layer Bi-directional GRU.\n",
        "- Take the final concatenated output representation from the GRU and pass it through a fully-connected layer to predict the final class probabilities (11 classes).\n",
        "- Use cross entropy as the loss function.\n",
        "- Conduct data augmentation to regularize the model.\n",
        "- Export the validation results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbe23c65",
      "metadata": {
        "id": "fbe23c65",
        "lines_to_next_cell": 2
      },
      "source": [
        "#### Dataset\n",
        "\n",
        "Let's start with the `FieldSequenceDataset` class.\n",
        "\n",
        "We have a series of images taken of a field over time, and each of these sequences of images corresponds to a `label` (e.g., the type of crop in the field). The `FieldSequenceDataset` class is designed to manage and provide easy access to these sequences of images and their corresponding labels.\n",
        "\n",
        "##### **Key Methods**\n",
        "- `__len__`: This returns the number of items in the dataset.\n",
        "- `__getitem__`: Given an index, it provides the images sequence and its label.\n",
        "- `plot`: lets you visualize a sequence of field images.\n",
        "\n",
        "##### **Why is this useful?**\n",
        "By structuring the data in this way, it becomes much easier to:\n",
        "- Feed data into machine learning models.\n",
        "- Apply consistent modifications to sequences.\n",
        "- Visualize and understand the data you're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb439327",
      "metadata": {
        "cellView": "form",
        "id": "cb439327"
      },
      "outputs": [],
      "source": [
        "# @title Implement the `__len__` and `__getitem__` methods\n",
        "class FieldSequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset class for sequences of field images.\n",
        "\n",
        "    Attributes:\n",
        "    - X: Numpy array containing image sequences.\n",
        "    - y: Labels associated with each image sequence.\n",
        "    - classes: List of class names/labels.\n",
        "    - transforms: Optional data augmentation operations.\n",
        "\n",
        "    Methods:\n",
        "    - __len__ : Returns the length of the dataset.\n",
        "    - __getitem__ : Fetches a data sample for a given index.\n",
        "    - plot: Plots an image sequence from a given sample.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, X, y, field_ids: List[int], transforms: Optional[Callable] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the dataset object.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Numpy array containing image sequences of shape (num_samples, num_images, height, width, bands).\n",
        "        - y: Numpy array containing labels for each sequence.\n",
        "        - field_ids: List of indices to subset the dataset. Defaults to None (use all data).\n",
        "        - transforms: Optional data augmentation operations.\n",
        "        \"\"\"\n",
        "\n",
        "        # Define class labels\n",
        "        self.classes = [str(i) for i in range(1, 8)]\n",
        "\n",
        "        # Instead of slicing the data, store the indices\n",
        "        self.field_ids = field_ids\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        # Set the data augmentation transforms\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, index: int) -> dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Returns a data sample given an index.\n",
        "\n",
        "        Parameters:\n",
        "        - index: Index of the sample to fetch.\n",
        "\n",
        "        Returns:\n",
        "        Dictionary containing the image sequence and its associated label.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def plot(\n",
        "        self,\n",
        "        sample: Dict[str, Any],\n",
        "        show_titles: bool = True,\n",
        "        suptitle: Optional[str] = None,\n",
        "    ) -> plt.Figure:\n",
        "        \"\"\"\n",
        "        Plots an image sequence from a sample.\n",
        "\n",
        "        Parameters:\n",
        "        - sample: Dictionary containing an image sequence and its label.\n",
        "        - show_titles: Whether to display titles on the plots.\n",
        "        - suptitle: Optional overarching title for the entire plot.\n",
        "\n",
        "        Returns:\n",
        "        Matplotlib figure object.\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract and normalize image sequence\n",
        "        sequence = sample[\"image\"].numpy()[:, [3, 2, 1], :, :]\n",
        "        label = sample[\"label\"].item()\n",
        "        min_vals = sequence.min(axis=(0, 2, 3), keepdims=True)\n",
        "        max_vals = sequence.max(axis=(0, 2, 3), keepdims=True)\n",
        "        sequence = (sequence - min_vals) / (max_vals - min_vals)\n",
        "\n",
        "        # Calculate layout for plotting multiple images\n",
        "        num_images = sequence.shape[0]\n",
        "        num_rows = int(np.ceil(num_images / 4.0))\n",
        "\n",
        "        # Create a figure and plot each image in the sequence\n",
        "        fig, axarr = plt.subplots(num_rows, 4, figsize=(15, 4 * num_rows))\n",
        "        if num_rows == 1:\n",
        "            axarr = np.expand_dims(axarr, axis=0)\n",
        "        for i in range(num_rows):\n",
        "            for j in range(4):\n",
        "                idx = i * 4 + j\n",
        "                if idx < num_images:\n",
        "                    ax = axarr[i, j]\n",
        "                    ax.imshow(sequence[idx].transpose(1, 2, 0))\n",
        "                    ax.axis(\"off\")\n",
        "                    if show_titles and idx == num_images - 1:\n",
        "                        ax.set_title(f\"Label: {self.classes[label]}\")\n",
        "                else:\n",
        "                    axarr[i, j].axis(\"off\")\n",
        "\n",
        "        # Set the optional overarching title\n",
        "        if suptitle:\n",
        "            fig.suptitle(suptitle, fontsize=16)\n",
        "\n",
        "        return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decfd121",
      "metadata": {
        "cellView": "form",
        "id": "decfd121",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "class FieldSequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset class for sequences of field images.\n",
        "\n",
        "    Attributes:\n",
        "    - X: Numpy array containing image sequences.\n",
        "    - y: Labels associated with each image sequence.\n",
        "    - classes: List of class names/labels.\n",
        "    - transforms: Optional data augmentation operations.\n",
        "\n",
        "    Methods:\n",
        "    - __len__ : Returns the length of the dataset.\n",
        "    - __getitem__ : Fetches a data sample for a given index.\n",
        "    - plot: Plots an image sequence from a given sample.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, X, y, field_ids: List[int], transforms: Optional[Callable] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the dataset object.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Numpy array containing image sequences of shape (num_samples, num_images, height, width, bands).\n",
        "        - y: Numpy array containing labels for each sequence.\n",
        "        - field_ids: List of indices to subset the dataset. Defaults to None (use all data).\n",
        "        - transforms: Optional data augmentation operations.\n",
        "        \"\"\"\n",
        "\n",
        "        # Define class labels\n",
        "        self.classes = [str(i) for i in range(1, 8)]\n",
        "\n",
        "        # Instead of slicing the data, store the indices\n",
        "        self.field_ids = field_ids\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        # Set the data augmentation transforms\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.field_ids)\n",
        "\n",
        "    def __getitem__(self, index: int) -> dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Returns a data sample given an index.\n",
        "\n",
        "        Parameters:\n",
        "        - index: Index of the sample to fetch.\n",
        "\n",
        "        Returns:\n",
        "        Dictionary containing the image sequence and its associated label.\n",
        "        \"\"\"\n",
        "\n",
        "        # Use the field_ids to fetch the relevant data\n",
        "        sequence = self.X[self.field_ids[index]]\n",
        "        label = self.y[self.field_ids[index]]\n",
        "\n",
        "        # Convert them to PyTorch tensors\n",
        "        sample = {\n",
        "            \"image\": torch.tensor(sequence, dtype=torch.float32),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def plot(\n",
        "        self,\n",
        "        sample: Dict[str, Any],\n",
        "        show_titles: bool = True,\n",
        "        suptitle: Optional[str] = None,\n",
        "    ) -> plt.Figure:\n",
        "        \"\"\"\n",
        "        Plots an image sequence from a sample.\n",
        "\n",
        "        Parameters:\n",
        "        - sample: Dictionary containing an image sequence and its label.\n",
        "        - show_titles: Whether to display titles on the plots.\n",
        "        - suptitle: Optional overarching title for the entire plot.\n",
        "\n",
        "        Returns:\n",
        "        Matplotlib figure object.\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract and normalize image sequence\n",
        "        sequence = sample[\"image\"].numpy()[:, [3, 2, 1], :, :]\n",
        "        label = sample[\"label\"].item()\n",
        "        min_vals = sequence.min(axis=(0, 2, 3), keepdims=True)\n",
        "        max_vals = sequence.max(axis=(0, 2, 3), keepdims=True)\n",
        "        sequence = (sequence - min_vals) / (max_vals - min_vals)\n",
        "\n",
        "        # Calculate layout for plotting multiple images\n",
        "        num_images = sequence.shape[0]\n",
        "        num_rows = int(np.ceil(num_images / 4.0))\n",
        "\n",
        "        # Create a figure and plot each image in the sequence\n",
        "        fig, axarr = plt.subplots(num_rows, 4, figsize=(15, 4 * num_rows))\n",
        "        if num_rows == 1:\n",
        "            axarr = np.expand_dims(axarr, axis=0)\n",
        "        for i in range(num_rows):\n",
        "            for j in range(4):\n",
        "                idx = i * 4 + j\n",
        "                if idx < num_images:\n",
        "                    ax = axarr[i, j]\n",
        "                    ax.imshow(sequence[idx].transpose(1, 2, 0))\n",
        "                    ax.axis(\"off\")\n",
        "                    if show_titles and idx == num_images - 1:\n",
        "                        ax.set_title(f\"Label: {self.classes[label]}\")\n",
        "                else:\n",
        "                    axarr[i, j].axis(\"off\")\n",
        "\n",
        "        # Set the optional overarching title\n",
        "        if suptitle:\n",
        "            fig.suptitle(suptitle, fontsize=16)\n",
        "\n",
        "        return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e56352",
      "metadata": {
        "id": "d0e56352",
        "lines_to_next_cell": 2
      },
      "source": [
        "#### Data Module\n",
        "\n",
        "In machine learning, it's common to split our data into three parts:\n",
        "- **Training set**: to train our model.\n",
        "- **Validation set**: to tune and optimize our model's parameters.\n",
        "- **Test set**: to check how well our model will perform in real-world scenarios.\n",
        "\n",
        "The next class we are going to implement (`FieldDataModule`) is a utility to handle and organize this process, especially for our field image sequences.\n",
        "\n",
        "##### **Why is this helpful?**\n",
        "- Organization: Keeps data management clean and structured.\n",
        "- Flexibility: Easily change the size of each dataset or the batch size.\n",
        "- Efficiency: Facilitates parallel data loading, so you spend less time waiting and more time training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e5a615",
      "metadata": {
        "cellView": "form",
        "id": "c9e5a615"
      },
      "outputs": [],
      "source": [
        "# @title Implement the `setup()`, `train_dataloader()`, `val_dataloader()`, and `test_dataloader()` methods\n",
        "class FieldDataModule(LightningDataModule):\n",
        "    \"\"\"\n",
        "    PyTorch Lightning data module for handling field sequence data.\n",
        "\n",
        "    This class helps in loading and splitting the dataset into train, validation, and test sets.\n",
        "\n",
        "    Attributes:\n",
        "    - root: The path to the root directory containing the data.\n",
        "    - batch_size: Size of the batches during training.\n",
        "    - workers: Number of workers for data loading.\n",
        "    - X: Numpy array containing image sequences.\n",
        "    - y: Numpy array containing labels for each sequence.\n",
        "    - train_ids, val_ids, test_ids: Lists containing indices for the train, validation, and test splits respectively.\n",
        "    - train_ds, val_ds, test_ds: Dataset objects for the train, validation, and test sets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        train_size: float = 0.7,\n",
        "        val_size: float = 0.2,\n",
        "        test_size: float = 0.1,\n",
        "        batch_size: int = 8,\n",
        "        workers: int = 4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define directory path and loading configurations\n",
        "        self.root = Path(root)\n",
        "        self.batch_size = batch_size\n",
        "        self.workers = workers\n",
        "\n",
        "        # Load the dataset into memory\n",
        "        self.X = np.load(self.root / \"X.npy\")\n",
        "        self.y = np.load(self.root / \"y.npy\")\n",
        "\n",
        "        # Randomly shuffle field IDs for dataset split\n",
        "        all_field_ids = list(range(3280))\n",
        "        shuffle(all_field_ids)\n",
        "\n",
        "        # Split the dataset into train, validation, and test sets based on provided ratios\n",
        "        self.train_ids, temp_ids = train_test_split(\n",
        "            all_field_ids, test_size=1 - train_size, random_state=42\n",
        "        )\n",
        "        self.val_ids, self.test_ids = train_test_split(\n",
        "            temp_ids, test_size=test_size / (test_size + val_size), random_state=42\n",
        "        )\n",
        "\n",
        "        # Setup datasets\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"\n",
        "        Prepare datasets for training, validation, and testing.\n",
        "\n",
        "        Uses the field IDs generated during initialization to subset the full dataset.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"Returns a DataLoader object for the training dataset.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"Returns a DataLoader object for the validation dataset.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        \"\"\"Returns a DataLoader object for the test dataset.\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09720d36",
      "metadata": {
        "cellView": "form",
        "id": "09720d36",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "class FieldDataModule(LightningDataModule):\n",
        "    \"\"\"\n",
        "    PyTorch Lightning data module for handling field sequence data.\n",
        "\n",
        "    This class helps in loading and splitting the dataset into train, validation, and test sets.\n",
        "\n",
        "    Attributes:\n",
        "    - root: The path to the root directory containing the data.\n",
        "    - batch_size: Size of the batches during training.\n",
        "    - workers: Number of workers for data loading.\n",
        "    - X: Numpy array containing image sequences.\n",
        "    - y: Numpy array containing labels for each sequence.\n",
        "    - train_ids, val_ids, test_ids: Lists containing indices for the train, validation, and test splits respectively.\n",
        "    - train_ds, val_ds, test_ds: Dataset objects for the train, validation, and test sets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        X: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        train_size: float = 0.7,\n",
        "        val_size: float = 0.2,\n",
        "        test_size: float = 0.1,\n",
        "        batch_size: int = 8,\n",
        "        workers: int = 4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define directory path and loading configurations\n",
        "        self.root = Path(root)\n",
        "        self.batch_size = batch_size\n",
        "        self.workers = workers\n",
        "\n",
        "        # Load the dataset into memory\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        # Randomly shuffle field IDs for dataset split\n",
        "        all_field_ids = list(range(len(y)))\n",
        "        shuffle(all_field_ids)\n",
        "\n",
        "        # Split the dataset into train, validation, and test sets based on provided ratios\n",
        "        self.train_ids, temp_ids = train_test_split(\n",
        "            all_field_ids, test_size=1 - train_size, random_state=42\n",
        "        )\n",
        "        self.val_ids, self.test_ids = train_test_split(\n",
        "            temp_ids, test_size=test_size / (test_size + val_size), random_state=42\n",
        "        )\n",
        "\n",
        "        # Setup datasets\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"\n",
        "        Prepare datasets for training, validation, and testing.\n",
        "\n",
        "        Uses the field IDs generated during initialization to subset the full dataset.\n",
        "        \"\"\"\n",
        "        self.train_ds = FieldSequenceDataset(self.X, self.y, self.train_ids)\n",
        "        self.val_ds = FieldSequenceDataset(self.X, self.y, self.val_ids)\n",
        "        self.test_ds = FieldSequenceDataset(self.X, self.y, self.test_ids)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"Returns a DataLoader object for the training dataset.\"\"\"\n",
        "        return DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.workers,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"Returns a DataLoader object for the validation dataset.\"\"\"\n",
        "        return DataLoader(\n",
        "            self.val_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.workers,\n",
        "            shuffle=False,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        \"\"\"Returns a DataLoader object for the test dataset.\"\"\"\n",
        "        return DataLoader(\n",
        "            self.test_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.workers,\n",
        "            shuffle=False,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75fde3e9",
      "metadata": {
        "id": "75fde3e9",
        "lines_to_next_cell": 2
      },
      "source": [
        "#### Model Architecture\n",
        "\n",
        "Now, to the model architecture. Imagine you have sequences of images captured over time, and you want to classify each sequence into a category. In our case, the sequence are images of a field, and we want to identify the crop grown in that field based on the sequence.\n",
        "\n",
        "##### **Main Components**\n",
        "- `ResNet18 Encoder`: Every image in the sequence is first processed by this encoder, which extracts important features from each image. Think of this as converting a detailed image into a summarized version that retains all the essential information.\n",
        "- `Bidirectional GRU`: Once we have the features for each image in the sequence, this component helps the model understand the order and relationship between these images. It looks at the sequence forwards and backwards, ensuring it captures patterns that emerge over time.\n",
        "- `Fully Connected Layer`: After understanding the sequence, this part of the model makes the final decision. It takes the output of the GRU and classifies the entire sequence into one of the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d23f6b17",
      "metadata": {
        "cellView": "form",
        "id": "d23f6b17"
      },
      "outputs": [],
      "source": [
        "# @title Implement the `forward()` method\n",
        "class SequenceClassificationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model for sequence classification tasks.\n",
        "\n",
        "    This model consists of a ResNet18 encoder, a bidirectional GRU, and a fully connected classifier.\n",
        "    Given an input sequence of images, it outputs class probabilities for each sequence.\n",
        "\n",
        "    Attributes:\n",
        "    - encoder: ResNet18 encoder for feature extraction from each image in the sequence.\n",
        "    - gru: Bidirectional GRU to model temporal dependencies in the sequence of features.\n",
        "    - fc: Fully connected layer to produce class probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        input_size: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        num_classes: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "\n",
        "        Parameters:\n",
        "        - in_channels (int): Number of channels in the input images.\n",
        "        - input_size (int): Size of the encoded image features, which serves as the input size for the GRU.\n",
        "        - hidden_size (int): Number of units in the hidden layer of the GRU.\n",
        "        - num_layers (int): Number of recurrent layers in the GRU.\n",
        "        - num_classes (int): Number of output classes for classification.\n",
        "        \"\"\"\n",
        "        super(SequenceClassificationModel, self).__init__()\n",
        "\n",
        "        # Create ResNet18 encoder using timm, configured for the specified number of input channels.\n",
        "        self.encoder = timm.create_model(\n",
        "            \"resnet18\",\n",
        "            num_classes=0,  # Setting to 0 removes the classification head.\n",
        "            in_chans=in_channels,\n",
        "            pretrained=True,\n",
        "        )\n",
        "\n",
        "        # Bidirectional GRU for modeling sequences.\n",
        "        self.gru = nn.GRU(\n",
        "            input_size, hidden_size, num_layers, bidirectional=True, batch_first=True\n",
        "        )\n",
        "\n",
        "        # Fully connected layer for outputting class probabilities.\n",
        "        self.fc = nn.Linear(\n",
        "            2 * hidden_size, num_classes\n",
        "        )  # 2 * hidden_size because the GRU is bidirectional.\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Parameters:\n",
        "        - x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, channels, height, width).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor of class probabilities with shape (batch_size, num_classes).\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44aff2ab",
      "metadata": {
        "cellView": "form",
        "id": "44aff2ab",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "class SequenceClassificationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model for sequence classification tasks.\n",
        "\n",
        "    This model consists of a ResNet18 encoder, a bidirectional GRU, and a fully connected classifier.\n",
        "    Given an input sequence of images, it outputs class probabilities for each sequence.\n",
        "\n",
        "    Attributes:\n",
        "    - encoder: ResNet18 encoder for feature extraction from each image in the sequence.\n",
        "    - gru: Bidirectional GRU to model temporal dependencies in the sequence of features.\n",
        "    - fc: Fully connected layer to produce class probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        input_size: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        num_classes: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "\n",
        "        Parameters:\n",
        "        - in_channels (int): Number of channels in the input images.\n",
        "        - input_size (int): Size of the encoded image features, which serves as the input size for the GRU.\n",
        "        - hidden_size (int): Number of units in the hidden layer of the GRU.\n",
        "        - num_layers (int): Number of recurrent layers in the GRU.\n",
        "        - num_classes (int): Number of output classes for classification.\n",
        "        \"\"\"\n",
        "        super(SequenceClassificationModel, self).__init__()\n",
        "\n",
        "        # Create ResNet18 encoder using timm, configured for the specified number of input channels.\n",
        "        self.encoder = timm.create_model(\n",
        "            \"resnet18\",\n",
        "            num_classes=0,  # Setting to 0 removes the classification head.\n",
        "            in_chans=in_channels,\n",
        "            pretrained=True,\n",
        "        )\n",
        "\n",
        "        # Bidirectional GRU for modeling sequences.\n",
        "        self.gru = nn.GRU(\n",
        "            input_size, hidden_size, num_layers, bidirectional=True, batch_first=True\n",
        "        )\n",
        "\n",
        "        # Fully connected layer for outputting class probabilities.\n",
        "        self.fc = nn.Linear(\n",
        "            2 * hidden_size, num_classes\n",
        "        )  # 2 * hidden_size because the GRU is bidirectional.\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Parameters:\n",
        "        - x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, channels, height, width).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor of class probabilities with shape (batch_size, num_classes).\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, C, H, W = x.shape\n",
        "\n",
        "        # Reshape to pass individual images through the encoder.\n",
        "        x = x.view(batch_size * seq_len, C, H, W)\n",
        "\n",
        "        # Pass through encoder to obtain encoded features.\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # Reshape back to sequence format for GRU.\n",
        "        x = x.view(batch_size, seq_len, -1)\n",
        "\n",
        "        # Pass through GRU. We only need the hidden state of the last layer.\n",
        "        _, h_n = self.gru(x)\n",
        "\n",
        "        # Reshape hidden states and concatenate the hidden states of the last layer's forward and backward passes.\n",
        "        h_n = h_n.view(self.gru.num_layers, 2, batch_size, self.gru.hidden_size)\n",
        "        final_hidden = torch.cat((h_n[-1, 0], h_n[-1, 1]), dim=-1)\n",
        "\n",
        "        # Pass through the classifier to get class probabilities.\n",
        "        output = self.fc(final_hidden)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e808d93",
      "metadata": {
        "id": "2e808d93",
        "lines_to_next_cell": 2
      },
      "source": [
        "#### Data Augmentation\n",
        "\n",
        "We design the `SequenceAugmentationPipeline` class to address the challenge of consistent augmentation across sequences. We also want to introduce variability (augmentation) in our data for better model training.\n",
        "\n",
        "##### **Key Components**\n",
        "- Random Horizontal Flip: This might mirror your image as if you're looking at its reflection in a pond.\n",
        "- Random Vertical Flip: Imagine flipping your photo upside-down.\n",
        "- Random Rotation: This slightly rotates your image, just as if you'd tilted your camera a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73910a76",
      "metadata": {
        "cellView": "form",
        "id": "73910a76"
      },
      "outputs": [],
      "source": [
        "# @title Implement the `forward()` method\n",
        "class SequenceAugmentationPipeline(nn.Module):\n",
        "    \"\"\"\n",
        "    A data augmentation pipeline for sequences of images.\n",
        "\n",
        "    This module defines a set of transformations that are applied across\n",
        "    all images in a sequence.\n",
        "\n",
        "    Attributes:\n",
        "    - hflip: Random horizontal flip transformation.\n",
        "    - vflip: Random vertical flip transformation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the augmentation pipeline with desired transformations.\n",
        "        \"\"\"\n",
        "        super(SequenceAugmentationPipeline, self).__init__()\n",
        "\n",
        "        self.hflip = K.RandomHorizontalFlip()\n",
        "        self.vflip = K.RandomVerticalFlip()\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply the transformations consistently across each image in the sequence.\n",
        "\n",
        "        Parameters:\n",
        "        - input (torch.Tensor): Input tensor of shape (batch_size, sequence_length, bands, height, width).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Augmented tensor with the same shape as input.\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b19477",
      "metadata": {
        "cellView": "form",
        "id": "56b19477",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "class SequenceAugmentationPipeline(nn.Module):\n",
        "    \"\"\"\n",
        "    A data augmentation pipeline for sequences of images.\n",
        "\n",
        "    This module defines a set of transformations that can be applied independently\n",
        "    to each image in a sequence, allowing for variability in augmentations across\n",
        "    the sequence.\n",
        "\n",
        "    Attributes:\n",
        "    - hflip: Random horizontal flip transformation.\n",
        "    - vflip: Random vertical flip transformation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the augmentation pipeline with desired transformations.\n",
        "        \"\"\"\n",
        "        super(SequenceAugmentationPipeline, self).__init__()\n",
        "\n",
        "        self.hflip = K.RandomHorizontalFlip()\n",
        "        self.vflip = K.RandomVerticalFlip()\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply the transformations independently across each image in the sequence.\n",
        "\n",
        "        Parameters:\n",
        "        - input (torch.Tensor): Input tensor of shape (batch_size, sequence_length, bands, height, width).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Augmented tensor with the same shape as input.\n",
        "        \"\"\"\n",
        "\n",
        "        # Apply the transformations to each image in the sequence.\n",
        "        transformed_seq = []\n",
        "        for image in input.unbind(dim=1):\n",
        "            hflip_params = self.hflip.forward_parameters(image.shape)\n",
        "            vflip_params = self.vflip.forward_parameters(image.shape)\n",
        "\n",
        "            image = self.hflip(image, hflip_params)\n",
        "            image = self.vflip(image, vflip_params)\n",
        "            transformed_seq.append(image)\n",
        "\n",
        "        # Combine the transformed images back into the sequence format.\n",
        "        output = torch.stack(transformed_seq, dim=1)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45a4a2b8",
      "metadata": {
        "id": "45a4a2b8",
        "lines_to_next_cell": 2
      },
      "source": [
        "#### Task\n",
        "\n",
        "The following class helps us manage the training process using Pytorch-Lightning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35d8e0b8",
      "metadata": {
        "cellView": "form",
        "id": "35d8e0b8"
      },
      "outputs": [],
      "source": [
        "# @title Create the task class\n",
        "class SequenceClassificationTask(LightningModule):\n",
        "    \"\"\"\n",
        "    Lightning module for the sequence classification task.\n",
        "\n",
        "    This module wraps the SequenceClassificationModel for training, validation, and testing.\n",
        "    It also handles data augmentation using the SequenceAugmentationPipeline.\n",
        "\n",
        "    Attributes:\n",
        "    - model: The sequence classification model.\n",
        "    - loss_fn: Loss function for classification.\n",
        "    - learning_rate: Learning rate for the optimizer.\n",
        "    - aug: Data augmentation pipeline for training sequences.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hidden_size,\n",
        "        in_channels=14,\n",
        "        num_layers=3,\n",
        "        num_classes=7,\n",
        "        learning_rate=0.001,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the lightning module.\n",
        "\n",
        "        Parameters:\n",
        "        - input_size (int): Size of the input to the GRU.\n",
        "        - hidden_size (int): Size of the GRU hidden state.\n",
        "        - in_channels (int, optional): Number of input channels to the model. Defaults to 14.\n",
        "        - num_layers (int, optional): Number of GRU layers. Defaults to 3.\n",
        "        - num_classes (int, optional): Number of classification classes. Defaults to 7.\n",
        "        - learning_rate (float, optional): Learning rate for the optimizer. Defaults to 0.001.\n",
        "        \"\"\"\n",
        "        super(SequenceClassificationTask, self).__init__()\n",
        "\n",
        "        self.model = SequenceClassificationModel(\n",
        "            in_channels, input_size, hidden_size, num_layers, num_classes\n",
        "        )\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Define the data augmentation pipeline for training.\n",
        "        self.aug = SequenceAugmentationPipeline()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Parameters:\n",
        "        - x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Model predictions.\n",
        "        \"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Defines a single step during training.\n",
        "\n",
        "        Parameters:\n",
        "        - batch (dict): Batch of data.\n",
        "        - batch_idx (int): Index of the batch.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Training loss.\n",
        "        \"\"\"\n",
        "        x, y = batch[\"image\"], batch[\"label\"]\n",
        "\n",
        "        # Apply data augmentation to the training data.\n",
        "        x = self.aug(x)\n",
        "\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "\n",
        "        # Log training loss to TensorBoard.\n",
        "        self.log(\"train_loss\", loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Defines a single step during validation.\n",
        "\n",
        "        Parameters:\n",
        "        - batch (dict): Batch of data.\n",
        "        - batch_idx (int): Index of the batch.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Validation loss.\n",
        "        \"\"\"\n",
        "        x, y = batch[\"image\"], batch[\"label\"]\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "\n",
        "        # Log validation loss to TensorBoard.\n",
        "        self.log(\"val_loss\", loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Defines a single step during testing.\n",
        "\n",
        "        Parameters:\n",
        "        - batch (dict): Batch of data.\n",
        "        - batch_idx (int): Index of the batch.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Testing loss.\n",
        "        \"\"\"\n",
        "        x, y = batch[\"image\"], batch[\"label\"]\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "\n",
        "        # Log testing loss to TensorBoard.\n",
        "        self.log(\"test_loss\", loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures the optimizer(s) and learning rate scheduler(s).\n",
        "\n",
        "        Returns:\n",
        "        - Dict: Contains optimizer and learning rate scheduler information.\n",
        "        \"\"\"\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Define a learning rate scheduler that reduces the learning rate when the validation loss plateaus.\n",
        "        scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"monitor\": \"val_loss\",\n",
        "            },\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa977b0",
      "metadata": {
        "id": "dfa977b0"
      },
      "source": [
        "#### **Training**\n",
        "\n",
        "We train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ede0cb5",
      "metadata": {
        "id": "3ede0cb5"
      },
      "outputs": [],
      "source": [
        "# Set training config\n",
        "root = \"./data\"\n",
        "experiment_name = \"seq2one-poc\"\n",
        "gpu = 0\n",
        "min_epochs, max_epochs = 3, 50\n",
        "\n",
        "# Set the hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "hidden_size = 128\n",
        "num_layers = 3\n",
        "early_stopping_patience = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "526dacd6",
      "metadata": {
        "id": "526dacd6"
      },
      "outputs": [],
      "source": [
        "num_classes = len(np.unique(y))\n",
        "in_channels = X.shape[1]  # bands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4476f640",
      "metadata": {
        "id": "4476f640"
      },
      "outputs": [],
      "source": [
        "# Create the data module\n",
        "dm = FieldDataModule(root=root, batch_size=batch_size, workers=2, X=X, y=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "596ae1cd",
      "metadata": {
        "id": "596ae1cd"
      },
      "outputs": [],
      "source": [
        "# Create the task with the sampled hyperparameters\n",
        "task = SequenceClassificationTask(\n",
        "    input_size=512,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        "    learning_rate=learning_rate,\n",
        "    num_classes=num_classes,\n",
        "    in_channels=in_channels,\n",
        ")\n",
        "\n",
        "# Create a dedicated models' directory for saving the trial's best models\n",
        "models_path = Path(f\"./models/{experiment_name}/\")\n",
        "models_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set the callbacks\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"val_loss\",\n",
        "    dirpath=models_path,\n",
        "    filename=\"model-{epoch:02d}-{val_loss:.2f}\",\n",
        "    save_top_k=1,\n",
        "    mode=\"min\",\n",
        ")\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\", mode=\"min\", patience=early_stopping_patience\n",
        ")\n",
        "\n",
        "# Create a TensorBoard logger\n",
        "logger = TensorBoardLogger(\"./tb_logs\", name=experiment_name)\n",
        "\n",
        "# Trainer definition\n",
        "trainer = Trainer(\n",
        "    logger=logger,\n",
        "    accelerator=\"gpu\",\n",
        "    devices=[gpu],\n",
        "    max_epochs=max_epochs,\n",
        "    min_epochs=min_epochs,\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "    precision=16,\n",
        ")\n",
        "\n",
        "trainer.fit(model=task, datamodule=dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289a5f2c",
      "metadata": {
        "id": "289a5f2c"
      },
      "source": [
        "Let's check the best validation score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "677c3fd1",
      "metadata": {
        "id": "677c3fd1"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback.best_model_score.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "043d57f6",
      "metadata": {
        "id": "043d57f6"
      },
      "source": [
        "Let's get the prediction of the deep learning model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef02f3c1",
      "metadata": {
        "id": "ef02f3c1"
      },
      "outputs": [],
      "source": [
        "# Load your model\n",
        "model = SequenceClassificationTask.load_from_checkpoint(\n",
        "    trainer.checkpoint_callback.best_model_path,\n",
        "    input_size=512,\n",
        "    hidden_size=hidden_size,\n",
        "    num_classes=num_classes,\n",
        "    in_channels=in_channels,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "model.freeze()\n",
        "\n",
        "# Get the validation data loader\n",
        "test_dl = dm.test_dataloader()\n",
        "\n",
        "# Predict\n",
        "all_logits = []\n",
        "y_tests = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dl:\n",
        "        inputs = batch[\"image\"].to(device)\n",
        "        y_test = batch[\"label\"]\n",
        "        logits = model(inputs)\n",
        "        all_logits.append(logits)\n",
        "        y_tests.append(y_test)\n",
        "\n",
        "# Concatenate all the results\n",
        "all_logits = torch.cat(all_logits, dim=0)\n",
        "y_test = torch.cat(y_tests, dim=0)\n",
        "\n",
        "# Get the probabilities\n",
        "y_test_hat = torch.nn.functional.softmax(all_logits, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dcfcb96",
      "metadata": {
        "id": "7dcfcb96"
      },
      "source": [
        "Let's calculate the final metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd9b26af",
      "metadata": {
        "id": "dd9b26af"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Get the arrays\n",
        "y_test_np = y_test.cpu().numpy()\n",
        "y_test_hat_np = y_test_hat.cpu().numpy()\n",
        "\n",
        "# Convert y_val to a binary label indicator format\n",
        "y_test_bin = label_binarize(y_test_np, classes=range(num_classes))\n",
        "\n",
        "cross_entropy = log_loss(y_test_bin, y_test_hat_np)\n",
        "print(\"Cross Entropy:\", cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fea25de",
      "metadata": {
        "id": "4fea25de"
      },
      "outputs": [],
      "source": [
        "y_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52dd249a",
      "metadata": {
        "id": "52dd249a"
      },
      "source": [
        "### Improve the Results - <font color='purple'>`Homework`</font>\n",
        "\n",
        "The neural network has not beaten the `LightGBM` model results. Attempt to improve the results, suggestions:\n",
        "\n",
        "- [ ] **Data augmentation**: try other data augmentation techniques: MixUp, random crops, Rotation, etc.\n",
        "- [ ] **Lighter architecture**: replace the `ResNet` encoder with a simpler CNN architecture with fewer parameters.\n",
        "- [ ] **Hyperparameter tuning**: try different values for hyperparameters like learning rate, batch size, etc.\n",
        "- [ ] **Preprocessing**: derive standardization parameters from: sequence-item level, all items in same sequence position, item-band-level, etc.\n",
        "- [ ] **Indices**: add the same indices used in `LightGBM` as extra bands to the numpy array.\n",
        "- [ ] **More data?**: for both models, we're missing some important data, like a digital elevation model or some kind of climatic input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b2b2057",
      "metadata": {
        "id": "8b2b2057"
      },
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, we've explored the fascinating world of geospatial machine learning, but let's be clear - the real hero of any ML project is the data. We started by understanding the fundamentals of geospatial data, both vector and raster, because you can't build a castle without a solid foundation.\n",
        "\n",
        "After carefully preparing and validating our data (the most crucial step!), we trained a LightGBM model and evaluated its performance. The results showed that with good data, even traditional ML models can deliver powerful insights. We then pushed further with a Seq2One neural network, demonstrating how deep learning can extract even more nuanced patterns from geospatial data.\n",
        "\n",
        "But here's the key takeaway: no matter how sophisticated your models become, they're only as good as the data you feed them. The real magic happens in the data collection, cleaning, and preparation stages. That's where 80% of your effort should go.\n",
        "\n",
        "As you continue your journey in geospatial ML, remember: focus first on getting high-quality, well-structured data. The models will follow. There's a whole universe of tools and techniques waiting to be explored, but they all depend on that critical foundation of good data.\n",
        "\n",
        "Keep learning, keep experimenting, and most importantly, keep your data clean and well-documented. That's the true path to making meaningful contributions in this field. Happy learning!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce06233",
      "metadata": {
        "id": "3ce06233"
      },
      "source": [
        "\n",
        "## Resources\n",
        "\n",
        "### Tutorials\n",
        "\n",
        "- [Let's map Africa! Intro to Geospatial Machine Learning](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/geospatial_machine_learning.ipynb)\n",
        "- [Geospatial Primer](https://github.com/Akramz/geospatial-primer).\n",
        "- [Introduction to Geospatial Data](https://colab.research.google.com/drive/1-85h5tEB0AJYT8xQ5H1wtSnCafXuLTHo#scrollTo=JDT5jUmCiTH-).\n",
        "- [Geospatial Data Analysis](https://colab.research.google.com/drive/1Yfkm63OV3eCtR3IVB-4owi2DJgj2Wd84).\n",
        "- [Geospatial Deep learning: Getting started with TorchGeo](https://pytorch.org/blog/geospatial-deep-learning-with-torchgeo/).\n",
        "- [Automating GIS-processes Course]((https://autogis-site.readthedocs.io/en/latest/))\n",
        "- [Geospatial Data with Python: Shapely and Fiona](https://macwright.com/2012/10/31/gis-with-python-shapely-fiona.html)\n",
        "- [Introduction to Raster Data Processing in Open Source Python](https://www.earthdatascience.org/courses/use-data-open-source-python/intro-raster-data-python/raster-data-processing/).\n",
        "- [XArray fundamental](https://rabernat.github.io/research_computing_2018/xarray.html).\n",
        "- [XArray tutorials](https://github.com/xarray-contrib/xarray-tutorial).\n",
        "- [Visualization: contextily tutorial](https://geopandas.org/en/stable/gallery/plotting_basemap_background.html).\n",
        "\n",
        "\n",
        "### Libraries\n",
        "\n",
        "- [Shapely](https://github.com/shapely/shapely).\n",
        "- [GeoPandas](https://github.com/geopandas/geopandas).\n",
        "- [Contextily](https://github.com/geopandas/contextily).\n",
        "- [Rasterio](https://github.com/rasterio/rasterio).\n",
        "- [Xarray](https://github.com/pydata/xarray).\n",
        "- [RioXarray](https://github.com/corteva/rioxarray).\n",
        "- [TorchGeo](https://github.com/microsoft/torchgeo).\n",
        "\n",
        "### Credits\n",
        "\n",
        "- Mapbiomass dataset: https://mapbiomas.org/\n",
        "- Indaba 2024: https://indaba.deeplearningindaba.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16b054d4",
      "metadata": {
        "id": "16b054d4"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/JRHLXCqDVyw2LqM49\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PSJDJSZ-sizN",
        "outputId": "60bd524b-3c8b-43f2-d3ad-cb2cae36b2f0"
      },
      "id": "PSJDJSZ-sizN",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe\n",
              "\tsrc=\"https://forms.gle/JRHLXCqDVyw2LqM49\",\n",
              "  width=\"80%\"\n",
              "\theight=\"1200px\" >\n",
              "\tLoading...\n",
              "</iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526f6cf6",
      "metadata": {
        "id": "526f6cf6"
      },
      "source": [
        "<img src=\"https://dataobservatory.net/images/logos/logo-do-2025.svg\" width=\"50%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![khipu-Logo-2023-bkg.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "jWEPEZSBsoBW"
      },
      "id": "jWEPEZSBsoBW"
    },
    {
      "cell_type": "markdown",
      "id": "68036a38",
      "metadata": {
        "id": "68036a38"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}